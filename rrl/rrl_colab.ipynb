{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uthvQCD6agB4",
        "outputId": "1523ed7e-d79d-444b-d9fe-ed30db5ebb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version:\n",
            "3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "sys.version_info(major=3, minor=11, micro=13, releaselevel='final', serial=0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(\"Python version:\")\n",
        "print(sys.version)        # Full version string\n",
        "print(sys.version_info)   # Structured version info\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nb_conda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cg_-wFAamSd",
        "outputId": "6a68c2f0-9d1a-4c55-804f-ecbf25d72276"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement nb_conda (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for nb_conda\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3.10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ3Mt1xCblma",
        "outputId": "eef187a6-0c70-4cb3-b923-de00c1dfb0cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.10).\n",
            "python3.10 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda update conda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGQTKweSbmle",
        "outputId": "f0e95fe7-b2ea-41b6-df17-e7702335b7c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-15 10:35:56--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.191.158, 104.16.32.241, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.191.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 159476510 (152M) [application/octet-stream]\n",
            "Saving to: â€˜Miniconda3-latest-Linux-x86_64.shâ€™\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>] 152.09M   151MB/s    in 1.0s    \n",
            "\n",
            "2025-07-15 10:35:58 (151 MB/s) - â€˜Miniconda3-latest-Linux-x86_64.shâ€™ saved [159476510/159476510]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - conda\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    certifi-2025.6.15          |  py313h06a4308_0         155 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         155 KB\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  certifi                         2025.4.26-py313h06a4308_0 --> 2025.6.15-py313h06a4308_0 \n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "                                                                        \n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\bdone\n",
            "Executing transaction: | \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.10/site-packages')"
      ],
      "metadata": {
        "id": "JJ2PflV0dB7k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create -n myenv python=3.10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdD_BprmeKhe",
        "outputId": "4ed2b995-11a8-4f9e-a137-3af33fc47b58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/myenv\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.10\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    python-3.10.18             |       h1a3bd86_0        26.5 MB\n",
            "    setuptools-78.1.1          |  py310h06a4308_0         1.7 MB\n",
            "    wheel-0.45.1               |  py310h06a4308_0         115 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        28.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6 \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0 \n",
            "  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0 \n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0 \n",
            "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0 \n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
            "  openssl            pkgs/main/linux-64::openssl-3.0.16-h5eee18b_0 \n",
            "  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2 \n",
            "  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1 \n",
            "  python             pkgs/main/linux-64::python-3.10.18-h1a3bd86_0 \n",
            "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py310h06a4308_0 \n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0 \n",
            "  tk                 pkgs/main/linux-64::tk-8.6.14-h993c535_1 \n",
            "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.45.1-py310h06a4308_0 \n",
            "  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1 \n",
            "  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0 \n",
            "  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0 \n",
            "  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1 \n",
            "  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1 \n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1 \n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.10.18       | 26.5 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "setuptools-78.1.1    | 1.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "wheel-0.45.1         | 115 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "python-3.10.18       | 26.5 MB   | :  11% 0.11124245449616309/1 [00:00<00:00,  1.11it/s]\n",
            "setuptools-78.1.1    | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "\n",
            "wheel-0.45.1         | 115 KB    | : 100% 1.0/1 [00:00<00:00,  8.65it/s]\u001b[A\u001b[A\n",
            "setuptools-78.1.1    | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "\n",
            "python-3.10.18       | 26.5 MB   | :  74% 0.7433821165537248/1 [00:00<00:00,  2.73it/s]\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "Preparing transaction: | \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate myenv\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AnshSinghal/DHARA.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vARj4fN5ph0C",
        "outputId": "a46cb804-a141-4daf-f6d3-d3360e38a1fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DHARA'...\n",
            "remote: Enumerating objects: 810, done.\u001b[K\n",
            "remote: Counting objects: 100% (810/810), done.\u001b[K\n",
            "remote: Compressing objects: 100% (800/800), done.\u001b[K\n",
            "remote: Total 810 (delta 9), reused 809 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (810/810), 11.82 MiB | 6.56 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "# Setup conda shell support\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "# Install cudatoolkit using conda and matching CuPy with pip\n",
        "conda install -y cudatoolkit=11.3\n",
        "pip install -q opennyai \"numpy<2\" cupy-cuda113\n",
        "\n",
        "# Run the test\n",
        "python -c \"\n",
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "from opennyai import Pipeline\n",
        "from opennyai.utils import Data\n",
        "\n",
        "import spacy\n",
        "spacy.require_gpu()\n",
        "\n",
        "# Paths\n",
        "input_dir = \"/content/DHARA/cases\"\n",
        "output_dir = \"/content/DHARA/rrl\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 1. Collect and read all .txt case files\n",
        "file_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\n",
        "texts = []\n",
        "doc_ids = []\n",
        "\n",
        "for path in file_paths:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        texts.append(f.read())\n",
        "        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n",
        "\n",
        "# 2. Wrap with OpenNyAI-compatible Data object\n",
        "data = Data(texts, use_gpu=True)\n",
        "\n",
        "# 3. Init pipeline (use GPU if available)\n",
        "pipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n",
        "\n",
        "# 4. Run RRL\n",
        "results = pipe(data)\n",
        "\n",
        "# 5. Save each result as individual JSON file\n",
        "for doc_id, result in zip(doc_ids, results):\n",
        "    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Inference complete. {len(results)} files saved to {output_dir}\")\n",
        "\n",
        "\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q8rm8ny8eOV0",
        "outputId": "1c88da4c-ae9f-47b7-85b8-f42f38ded0a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/myenv\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit=11.3\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cudatoolkit-11.3.1         |       h2bc3f7f_2       549.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       549.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "                                                                        \n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\bdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m795.1/795.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m141.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m138.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m151.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91/91\u001b[0m [opennyai]\n",
            "\u001b[1A\u001b[2K/bin/bash: -c: line 52: syntax error near unexpected token `('\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'set -e\n\n# Setup conda shell support\neval \"$(conda shell.bash hook)\"\nconda activate myenv\n\n# Install cudatoolkit using conda and matching CuPy with pip\nconda install -y cudatoolkit=11.3\npip install -q opennyai \"numpy<2\" cupy-cuda113\n\n# Run the test\npython -c \"\nimport os\nimport json\nfrom glob import glob\nfrom opennyai import Pipeline\nfrom opennyai.utils import Data\n\nimport spacy\nspacy.require_gpu()\n\n# Paths\ninput_dir = \"/content/DHARA/cases\"\noutput_dir = \"/content/DHARA/rrl\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 1. Collect and read all .txt case files\nfile_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\ntexts = []\ndoc_ids = []\n\nfor path in file_paths:\n    with open(path, 'r', encoding='utf-8') as f:\n        texts.append(f.read())\n        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n\n# 2. Wrap with OpenNyAI-compatible Data object\ndata = Data(texts, use_gpu=True)\n\n# 3. Init pipeline (use GPU if available)\npipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n\n# 4. Run RRL\nresults = pipe(data)\n\n# 5. Save each result as individual JSON file\nfor doc_id, result in zip(doc_ids, results):\n    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(result, f, ensure_ascii=False, indent=2)\n\nprint(f\"âœ… Inference complete. {len(results)} files saved to {output_dir}\")\n\n\"\n' returned non-zero exit status 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3070033928.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -e\\n\\n# Setup conda shell support\\neval \"$(conda shell.bash hook)\"\\nconda activate myenv\\n\\n# Install cudatoolkit using conda and matching CuPy with pip\\nconda install -y cudatoolkit=11.3\\npip install -q opennyai \"numpy<2\" cupy-cuda113\\n\\n# Run the test\\npython -c \"\\nimport os\\nimport json\\nfrom glob import glob\\nfrom opennyai import Pipeline\\nfrom opennyai.utils import Data\\n\\nimport spacy\\nspacy.require_gpu()\\n\\n# Paths\\ninput_dir = \"/content/DHARA/cases\"\\noutput_dir = \"/content/DHARA/rrl\"\\nos.makedirs(output_dir, exist_ok=True)\\n\\n# 1. Collect and read all .txt case files\\nfile_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\\ntexts = []\\ndoc_ids = []\\n\\nfor path in file_paths:\\n    with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n        texts.append(f.read())\\n        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\\n\\n# 2. Wrap with OpenNyAI-compatible Data object\\ndata = Data(texts, use_gpu=True)\\n\\n# 3. Init pipeline (use GPU if available)\\npipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\\n\\n# 4. Run RRL\\nresults = pipe(data)\\n\\n# 5. Save each result as individual JSON file\\nfor doc_id, result in zip(doc_ids, results):\\n    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\\n        json.dump(res...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m    138\u001b[0m           \u001b[0mreturncode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       )\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'set -e\n\n# Setup conda shell support\neval \"$(conda shell.bash hook)\"\nconda activate myenv\n\n# Install cudatoolkit using conda and matching CuPy with pip\nconda install -y cudatoolkit=11.3\npip install -q opennyai \"numpy<2\" cupy-cuda113\n\n# Run the test\npython -c \"\nimport os\nimport json\nfrom glob import glob\nfrom opennyai import Pipeline\nfrom opennyai.utils import Data\n\nimport spacy\nspacy.require_gpu()\n\n# Paths\ninput_dir = \"/content/DHARA/cases\"\noutput_dir = \"/content/DHARA/rrl\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 1. Collect and read all .txt case files\nfile_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\ntexts = []\ndoc_ids = []\n\nfor path in file_paths:\n    with open(path, 'r', encoding='utf-8') as f:\n        texts.append(f.read())\n        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n\n# 2. Wrap with OpenNyAI-compatible Data object\ndata = Data(texts, use_gpu=True)\n\n# 3. Init pipeline (use GPU if available)\npipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n\n# 4. Run RRL\nresults = pipe(data)\n\n# 5. Save each result as individual JSON file\nfor doc_id, result in zip(doc_ids, results):\n    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(result, f, ensure_ascii=False, indent=2)\n\nprint(f\"âœ… Inference complete. {len(results)} files saved to {output_dir}\")\n\n\"\n' returned non-zero exit status 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"run_inference.py\", \"w\") as f:\n",
        "    f.write('''\\\n",
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "from opennyai import Pipeline\n",
        "from opennyai.utils import Data\n",
        "import spacy\n",
        "\n",
        "spacy.require_gpu()\n",
        "\n",
        "input_dir = \"/content/DHARA/cases\"\n",
        "output_dir = \"/content/DHARA/rrl\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "file_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\n",
        "texts = []\n",
        "doc_ids = []\n",
        "\n",
        "for path in file_paths:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        texts.append(f.read())\n",
        "        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n",
        "\n",
        "print(f\"ðŸ“„ Loaded {len(texts)} documents...\", flush=True)\n",
        "\n",
        "data = Data(texts, use_gpu=True)\n",
        "pipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n",
        "results = pipe(data)\n",
        "\n",
        "for doc_id, result in zip(doc_ids, results):\n",
        "    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"âœ… Saved: {output_path}\", flush=True)\n",
        "\n",
        "print(f\"ðŸ Inference complete. {len(results)} files saved to {output_dir}\", flush=True)\n",
        "''')\n"
      ],
      "metadata": {
        "id": "d8emJ-Ypr1I6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "set -e\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "# Run Python file using conda env interpreter\n",
        "python run_inference.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1seSgtHse4W",
        "outputId": "b39a6192-2331-4431-f591-5fff020fbc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/envs/myenv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "ðŸ“„ Loaded 400 documents...\n",
            "\u001b[38;5;4mâ„¹ Installing en_core_web_trf. This is a one time process!!\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Pre-processing will happen on GPU!\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Loading Rhetorical Role...\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Rhetorical Roles will use GPU!\u001b[0m\n",
            "Downloading: 100% 570/570 [00:00<00:00, 2.56MB/s]\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 64.9MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading: \"https://huggingface.co/opennyaiorg/InRhetoricalRoles/resolve/main/InRhetoricalRoleModel.pt\" to /root/.opennyai/rhetoricalrole/InRhetoricalRoleModel.pt\n",
            "100% 998M/998M [00:44<00:00, 23.8MB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 28.1MB/s]\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 218kB/s]\n",
            "\u001b[38;5;4mâ„¹ Preprocessing rhetorical role model input!!!\u001b[0m\n",
            "  0% 1/400 [00:17<1:55:17, 17.34s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
            "100% 400/400 [31:49<00:00,  4.77s/it]\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "\u001b[38;5;4mâ„¹ Processing documents with rhetorical role model!!!\u001b[0m\n",
            " 42% 170/400 [09:47<06:54,  1.80s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "# Install Miniconda only once\n",
        "if [ ! -f /usr/local/bin/conda ]; then\n",
        "  wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "  bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "fi\n",
        "\n",
        "# Initialize conda shell support\n",
        "source /usr/local/etc/profile.d/conda.sh\n",
        "\n",
        "# Create environment (if not already created)\n",
        "if ! conda info --envs | grep -q myenv; then\n",
        "  conda create -y -n myenv python=3.10\n",
        "fi\n",
        "\n",
        "# Install packages using full path to conda Python binary\n",
        "/usr/local/envs/myenv/bin/pip install -q opennyai \"spacy[cuda12x]\" cupy-cuda12x \"numpy<2\"\n",
        "\n",
        "# Run the test using the same Python interpreter\n",
        "/usr/local/envs/myenv/bin/python -c \"\n",
        "import spacy\n",
        "import cupy\n",
        "gpu = spacy.prefer_gpu()\n",
        "print(f'spaCy GPU support: {gpu}')\n",
        "if gpu:\n",
        "    print(f'âœ… CuPy using GPU ID: {cupy.cuda.runtime.getDevice()}')\n",
        "else:\n",
        "    print('âŒ spaCy is not using GPU')\n",
        "\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKoRLYsVhOdK",
        "outputId": "8bb6e860-b83e-443c-80a6-89c4d20740c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/myenv\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.10\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    python-3.10.18             |       h1a3bd86_0        26.5 MB\n",
            "    setuptools-78.1.1          |  py310h06a4308_0         1.7 MB\n",
            "    wheel-0.45.1               |  py310h06a4308_0         115 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        28.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6 \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0 \n",
            "  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0 \n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0 \n",
            "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0 \n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
            "  openssl            pkgs/main/linux-64::openssl-3.0.16-h5eee18b_0 \n",
            "  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2 \n",
            "  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1 \n",
            "  python             pkgs/main/linux-64::python-3.10.18-h1a3bd86_0 \n",
            "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py310h06a4308_0 \n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0 \n",
            "  tk                 pkgs/main/linux-64::tk-8.6.14-h993c535_1 \n",
            "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.45.1-py310h06a4308_0 \n",
            "  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1 \n",
            "  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0 \n",
            "  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0 \n",
            "  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1 \n",
            "  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1 \n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages: ...working...\rpython-3.10.18       | 26.5 MB   |            |   0% \n",
            "\rsetuptools-78.1.1    | 1.7 MB    |            |   0% \u001b[A\n",
            "\n",
            "\rwheel-0.45.1         | 115 KB    |            |   0% \u001b[A\u001b[A\n",
            "\n",
            "\rwheel-0.45.1         | 115 KB    | ########## | 100% \u001b[A\u001b[A\rpython-3.10.18       | 26.5 MB   | 1          |   1% \n",
            "\rsetuptools-78.1.1    | 1.7 MB    | 9          |   9% \u001b[A\n",
            "\n",
            "\rwheel-0.45.1         | 115 KB    | ########## | 100% \u001b[A\u001b[A\n",
            "\n",
            "\rwheel-0.45.1         | 115 KB    | ########## | 100% \u001b[A\u001b[A\n",
            "\rsetuptools-78.1.1    | 1.7 MB    | ########## | 100% \u001b[A\rpython-3.10.18       | 26.5 MB   | ###4       |  35% \rpython-3.10.18       | 26.5 MB   | #######2   |  72% \n",
            "\rsetuptools-78.1.1    | 1.7 MB    | ########## | 100% \u001b[A\n",
            "\rsetuptools-78.1.1    | 1.7 MB    | ########## | 100% \u001b[A\rpython-3.10.18       | 26.5 MB   | ########## | 100% \rpython-3.10.18       | 26.5 MB   | ########## | 100% \r                                                     \r\n",
            "\r                                                     \u001b[A\n",
            "\n",
            "\r                                                     \u001b[A\u001b[A done\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate myenv\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 18.2/18.2 MB 26.8 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.1/3.1 MB 53.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.9/12.9 MB 84.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.1/6.1 MB 91.9 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10.2/10.2 MB 111.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 795.1/795.1 kB 26.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.1/1.1 MB 48.8 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 659.5/659.5 kB 29.9 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 112.4/112.4 MB 61.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.4/5.4 MB 115.0 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 51.0 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.5/1.5 MB 63.6 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.3/12.3 MB 110.6 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 781.7/781.7 kB 40.6 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 37.7/37.7 MB 152.5 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.4/4.4 MB 130.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.1/3.1 MB 123.4 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.6/6.6 MB 152.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 751.2/751.2 kB 43.5 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 670.2/670.2 MB 39.8 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 410.6/410.6 MB 33.9 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14.1/14.1 MB 153.6 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 23.7/23.7 MB 175.9 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 823.6/823.6 kB 38.4 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 731.7/731.7 MB 21.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 121.6/121.6 MB 65.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 56.5/56.5 MB 59.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 124.2/124.2 MB 69.1 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 196.0/196.0 MB 72.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 209.8/209.8 MB 71.4 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 89.2/89.2 MB 61.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.8/13.8 MB 129.8 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.7/1.7 MB 73.6 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 39.7/39.7 MB 56.6 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 897.5/897.5 kB 37.6 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 57.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.3/6.3 MB 93.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 536.2/536.2 kB 17.1 MB/s eta 0:00:00\n",
            "\n",
            "spaCy GPU support: False\n",
            "âŒ spaCy is not using GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "WARNING: spacy 3.2.6 does not provide the extra 'cuda12x'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python -c \"\n",
        "import spacy\n",
        "import cupy\n",
        "print('spaCy GPU support:', spacy.prefer_gpu())\n",
        "print('CuPy GPU device:', cupy.cuda.runtime.getDevice())\n",
        "# \"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "2fTGEULZemxk",
        "outputId": "ba312906-daa5-47a5-8399-9b26a287ab36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy GPU support: False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CUDARuntimeError",
          "evalue": "cudaErrorInsufficientDriver: CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-1543160697.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spaCy GPU support:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefer_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CuPy GPU device:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/api/runtime.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.runtime.getDevice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/api/runtime.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.runtime.getDevice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/api/runtime.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.runtime.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorInsufficientDriver: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3JkPyLlgPep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}