{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:25:10.002164Z",
     "iopub.status.busy": "2025-07-15T10:25:10.001822Z",
     "iopub.status.idle": "2025-07-15T10:25:34.656008Z",
     "shell.execute_reply": "2025-07-15T10:25:34.655296Z",
     "shell.execute_reply.started": "2025-07-15T10:25:10.002135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX=/usr/local\n",
      "Unpacking payload ...\n",
      "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "\n",
      "Installing base environment...\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "installation finished.\n",
      "WARNING:\n",
      "    You currently have a PYTHONPATH environment variable set. This may cause\n",
      "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
      "    For best results, please verify that your PYTHONPATH only points to\n",
      "    directories of packages that are compatible with the Python interpreter\n",
      "    in Miniconda3: /usr/local\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2025.6.15          |  py313h06a4308_0         155 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         155 KB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi                         2025.4.26-py313h06a4308_0 --> 2025.6.15-py313h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/envs/myenv\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.10\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
      "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
      "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6 \n",
      "  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0 \n",
      "  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0 \n",
      "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0 \n",
      "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n",
      "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
      "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
      "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
      "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
      "  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0 \n",
      "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
      "  openssl            pkgs/main/linux-64::openssl-3.0.16-h5eee18b_0 \n",
      "  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2 \n",
      "  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1 \n",
      "  python             pkgs/main/linux-64::python-3.10.18-h1a3bd86_0 \n",
      "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
      "  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py310h06a4308_0 \n",
      "  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0 \n",
      "  tk                 pkgs/main/linux-64::tk-8.6.14-h993c535_1 \n",
      "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n",
      "  wheel              pkgs/main/linux-64::wheel-0.45.1-py310h06a4308_0 \n",
      "  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1 \n",
      "  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0 \n",
      "  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0 \n",
      "  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1 \n",
      "  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1 \n",
      "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate myenv\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and install Miniconda\n",
    "!wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "\n",
    "# Update conda\n",
    "!eval \"$(/usr/local/bin/conda shell.bash hook)\" && conda update -y -n base -c defaults conda\n",
    "\n",
    "# (Optional) Append site-packages for later Python access (matches your logic)\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.10/site-packages')\n",
    "\n",
    "# Create the Python 3.10 conda environment\n",
    "!eval \"$(/usr/local/bin/conda shell.bash hook)\" && conda create -y -n myenv python=3.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:25:44.008204Z",
     "iopub.status.busy": "2025-07-15T10:25:44.007574Z",
     "iopub.status.idle": "2025-07-15T10:28:43.022741Z",
     "shell.execute_reply": "2025-07-15T10:28:43.021910Z",
     "shell.execute_reply.started": "2025-07-15T10:25:44.008173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/envs/myenv\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=11.3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cudatoolkit-11.3.1         |       h2bc3f7f_2       549.3 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       549.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2 \n",
      "\n",
      "\n",
      "\n",
      " done                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: | By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 83.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 161.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/6.1 MB 106.0 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 132.0 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 795.1/795.1 kB 29.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 43.9 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 659.5/659.5 kB 24.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 42.0 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 146.7 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 125.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 49.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 57.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 155.7 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.7/781.7 kB 31.0 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.7/37.7 MB 151.0 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 106.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 94.7 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 102.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 751.2/751.2 kB 26.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.2/670.2 MB 53.4 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 68.9 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 139.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 148.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 32.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 30.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 117.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 124.5 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 55.3 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 33.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 MB 65.4 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.2/89.2 MB 70.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 81.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 47.7 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.7/39.7 MB 80.0 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 897.5/897.5 kB 30.4 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 40.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 79.9 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 18.6 MB/s eta 0:00:00\n",
      "\n",
      "spaCy GPU support: True\n",
      "CuPy GPU device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/myenv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "# Load conda into shell\n",
    "source /usr/local/etc/profile.d/conda.sh\n",
    "conda activate myenv\n",
    "\n",
    "# Install cudatoolkit and pip packages (exact versions)\n",
    "conda install -y cudatoolkit=11.3\n",
    "pip install -q opennyai \"numpy<2\" cupy-cuda113\n",
    "\n",
    "# Run GPU test\n",
    "python -c \"\n",
    "import spacy\n",
    "import cupy\n",
    "print('spaCy GPU support:', spacy.prefer_gpu())\n",
    "print('CuPy GPU device:', cupy.cuda.runtime.getDevice())\n",
    "\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T11:35:07.326207Z",
     "iopub.status.busy": "2025-07-15T11:35:07.325880Z",
     "iopub.status.idle": "2025-07-15T11:35:07.332518Z",
     "shell.execute_reply": "2025-07-15T11:35:07.331765Z",
     "shell.execute_reply.started": "2025-07-15T11:35:07.326181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with open(\"run_inference.py\", \"w\") as f:\n",
    "#     f.write('''\\\n",
    "# import os\n",
    "# import json\n",
    "# from glob import glob\n",
    "# from opennyai import Pipeline\n",
    "# from opennyai.utils import Data\n",
    "# import spacy\n",
    "\n",
    "# spacy.require_gpu()\n",
    "\n",
    "# input_dir = \"/kaggle/working/DHARA/cases\"\n",
    "# output_dir = \"rrl\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# file_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\n",
    "# texts = []\n",
    "# doc_ids = []\n",
    "\n",
    "# for path in file_paths:\n",
    "#     with open(path, 'r', encoding='utf-8') as f:\n",
    "#         texts.append(f.read())\n",
    "#         doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n",
    "\n",
    "# print(f\"📄 Loaded {len(texts)} documents...\", flush=True)\n",
    "\n",
    "# data = Data(texts, use_gpu=True)\n",
    "# pipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n",
    "# results = pipe(data)\n",
    "\n",
    "# for doc_id, result in zip(doc_ids, results):\n",
    "#     output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n",
    "#     with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"✅ Saved: {output_path}\", flush=True)\n",
    "\n",
    "# print(f\"🏁 Inference complete. {len(results)} files saved to {output_dir}\", flush=True)\n",
    "# ''')\n",
    "\n",
    "with open(\"run_inference.py\", \"w\") as f:\n",
    "    f.write('''\\\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from opennyai import Pipeline\n",
    "from opennyai.utils import Data\n",
    "import spacy\n",
    "\n",
    "spacy.require_gpu()\n",
    "\n",
    "input_dir = \"/kaggle/working/DHARA/cases\"\n",
    "output_dir = \"rrl\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load only case90 to case99\n",
    "file_paths = [\n",
    "    os.path.join(input_dir, f\"case{i}.txt\") for i in range(90, 100)\n",
    "    if os.path.exists(os.path.join(input_dir, f\"case{i}.txt\"))\n",
    "]\n",
    "print(file_paths)\n",
    "\n",
    "texts = []\n",
    "doc_ids = []\n",
    "\n",
    "for path in file_paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        texts.append(f.read())\n",
    "        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n",
    "\n",
    "print(f\"📄 Loaded {len(texts)} documents (case90–case99)...\", flush=True)\n",
    "\n",
    "data = Data(texts, use_gpu=True)\n",
    "pipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n",
    "results = pipe(data)\n",
    "\n",
    "for doc_id, result in zip(doc_ids, results):\n",
    "    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✅ Saved: {output_path}\", flush=True)\n",
    "\n",
    "print(f\"🏁 Inference complete. {len(results)} files saved to {output_dir}\", flush=True)\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T11:35:14.076924Z",
     "iopub.status.busy": "2025-07-15T11:35:14.076657Z",
     "iopub.status.idle": "2025-07-15T11:35:52.288497Z",
     "shell.execute_reply": "2025-07-15T11:35:52.287839Z",
     "shell.execute_reply.started": "2025-07-15T11:35:14.076905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/myenv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "['/kaggle/working/DHARA/cases/case90.txt', '/kaggle/working/DHARA/cases/case91.txt', '/kaggle/working/DHARA/cases/case92.txt', '/kaggle/working/DHARA/cases/case93.txt', '/kaggle/working/DHARA/cases/case94.txt', '/kaggle/working/DHARA/cases/case95.txt', '/kaggle/working/DHARA/cases/case96.txt', '/kaggle/working/DHARA/cases/case97.txt', '/kaggle/working/DHARA/cases/case98.txt', '/kaggle/working/DHARA/cases/case99.txt']\n",
      "📄 Loaded 10 documents (case90–case99)...\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on GPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use GPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:02<00:20,  2.32s/it]\n",
      " 20%|██        | 2/10 [00:04<00:16,  2.10s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      " 30%|███       | 3/10 [00:05<00:12,  1.81s/it]\n",
      " 40%|████      | 4/10 [00:07<00:10,  1.72s/it]\n",
      " 50%|█████     | 5/10 [00:09<00:10,  2.05s/it]\n",
      " 60%|██████    | 6/10 [00:11<00:07,  1.81s/it]\n",
      " 70%|███████   | 7/10 [00:13<00:05,  1.83s/it]\n",
      " 80%|████████  | 8/10 [00:14<00:03,  1.82s/it]\n",
      " 90%|█████████ | 9/10 [00:16<00:01,  1.73s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.58s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.78s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:01<00:11,  1.29s/it]\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.16s/it]\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.09s/it]\n",
      " 40%|████      | 4/10 [00:04<00:06,  1.05s/it]\n",
      " 50%|█████     | 5/10 [00:05<00:04,  1.03it/s]\n",
      " 60%|██████    | 6/10 [00:05<00:03,  1.10it/s]\n",
      " 70%|███████   | 7/10 [00:06<00:02,  1.17it/s]\n",
      " 80%|████████  | 8/10 [00:07<00:01,  1.27it/s]\n",
      " 90%|█████████ | 9/10 [00:07<00:00,  1.36it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.48it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n",
      "✅ Saved: rrl/case90.json\n",
      "✅ Saved: rrl/case91.json\n",
      "✅ Saved: rrl/case92.json\n",
      "✅ Saved: rrl/case93.json\n",
      "✅ Saved: rrl/case94.json\n",
      "✅ Saved: rrl/case95.json\n",
      "✅ Saved: rrl/case96.json\n",
      "✅ Saved: rrl/case97.json\n",
      "✅ Saved: rrl/case98.json\n",
      "✅ Saved: rrl/case99.json\n",
      "🏁 Inference complete. 10 files saved to rrl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Path to the Conda env’s Python\n",
    "python_path = \"/usr/local/envs/myenv/bin/python\"\n",
    "\n",
    "# Run script with live logs\n",
    "process = subprocess.Popen(\n",
    "    [python_path, \"run_inference.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1,\n",
    ")\n",
    "\n",
    "for line in process.stdout:\n",
    "    print(line, end=\"\")  # Prints each line live\n",
    "\n",
    "process.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T11:36:43.327105Z",
     "iopub.status.busy": "2025-07-15T11:36:43.326280Z",
     "iopub.status.idle": "2025-07-15T11:36:45.905047Z",
     "shell.execute_reply": "2025-07-15T11:36:45.904170Z",
     "shell.execute_reply.started": "2025-07-15T11:36:43.327074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/rrl/ (stored 0%)\n",
      "  adding: kaggle/working/rrl/case139.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case2.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case25.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case93.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case361.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case55.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case245.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case215.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case41.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case336.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case135.json (deflated 84%)\n",
      "  adding: kaggle/working/rrl/case376.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case311.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case297.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case141.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case199.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case356.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case99.json (deflated 70%)\n",
      "  adding: kaggle/working/rrl/case269.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case19.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case138.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case340.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case29.json (deflated 69%)\n",
      "  adding: kaggle/working/rrl/case153.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case69.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case11.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case86.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case7.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case363.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case357.json (deflated 83%)\n",
      "  adding: kaggle/working/rrl/case252.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case203.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case324.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case371.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case173.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case386.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case292.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case127.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case242.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case168.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case144.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case351.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case152.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case368.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case279.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case184.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case321.json (deflated 79%)\n",
      "  adding: kaggle/working/rrl/case331.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case68.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case341.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case296.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case222.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case335.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case383.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case114.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case164.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case5.json (deflated 82%)\n",
      "  adding: kaggle/working/rrl/case400.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case33.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case320.json (deflated 79%)\n",
      "  adding: kaggle/working/rrl/case185.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case117.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case195.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case396.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case136.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case323.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case156.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case56.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case308.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case190.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case263.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case65.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case288.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case268.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case90.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case61.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case378.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case255.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case375.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case42.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case247.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case63.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case48.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case249.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case332.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case124.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case370.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case298.json (deflated 70%)\n",
      "  adding: kaggle/working/rrl/case122.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case374.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case104.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case381.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case22.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case399.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case236.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case262.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case275.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case98.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case14.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case291.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case326.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case343.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case106.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case282.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case111.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case82.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case319.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case183.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case79.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case73.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case193.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case309.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case8.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case272.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case85.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case228.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case230.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case278.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case180.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case348.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case281.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case346.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case167.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case237.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case314.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case52.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case108.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case393.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case295.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case23.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case84.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case334.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case160.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case140.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case60.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case121.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case116.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case134.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case38.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case360.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case206.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case15.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case207.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case109.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case174.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case105.json (deflated 82%)\n",
      "  adding: kaggle/working/rrl/case17.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case307.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case344.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case310.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case202.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case276.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case266.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case328.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case306.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case97.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case355.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case240.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case107.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case187.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case115.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case216.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case189.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case188.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case149.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case50.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case214.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case155.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case182.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case347.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case337.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case353.json (deflated 82%)\n",
      "  adding: kaggle/working/rrl/case208.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case283.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case345.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case257.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case75.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case390.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case220.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case253.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case299.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case21.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case43.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case36.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case395.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case226.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case250.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case71.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case229.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case113.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case273.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case165.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case342.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case251.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case150.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case367.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case163.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case157.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case274.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case315.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case20.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case286.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case339.json (deflated 82%)\n",
      "  adding: kaggle/working/rrl/case67.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case285.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case377.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case318.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case284.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case92.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case198.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case175.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case70.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case132.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case172.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case191.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case212.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case31.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case317.json (deflated 83%)\n",
      "  adding: kaggle/working/rrl/case146.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case128.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case362.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case303.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case186.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case197.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case78.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case119.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case161.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case277.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case125.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case325.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case235.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case49.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case234.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case126.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case397.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case89.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case196.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case241.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case352.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case27.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case290.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case384.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case238.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case219.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case287.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case205.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case35.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case349.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case142.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case170.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case391.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case94.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case333.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case192.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case131.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case302.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case389.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case57.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case210.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case176.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case300.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case58.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case248.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case87.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case145.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case101.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case103.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case261.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case338.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case39.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case232.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case40.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case233.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case169.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case217.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case366.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case322.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case280.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case388.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case159.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case4.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case264.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case329.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case260.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case9.json (deflated 70%)\n",
      "  adding: kaggle/working/rrl/case96.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case12.json (deflated 83%)\n",
      "  adding: kaggle/working/rrl/case254.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case201.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case46.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case394.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case294.json (deflated 70%)\n",
      "  adding: kaggle/working/rrl/case265.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case95.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case53.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case289.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case133.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case213.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case385.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case112.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case223.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case10.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case166.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case380.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case313.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case256.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case162.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case74.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case271.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case358.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case80.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case83.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case177.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case354.json (deflated 82%)\n",
      "  adding: kaggle/working/rrl/case81.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case258.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case118.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case129.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case72.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case270.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case147.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case246.json (deflated 70%)\n",
      "  adding: kaggle/working/rrl/case151.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case231.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case77.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case148.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case143.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case26.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case91.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case30.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case110.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case379.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case364.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case327.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case211.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case218.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case130.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case316.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case382.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case88.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case120.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case243.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case28.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case45.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case365.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case259.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case225.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case102.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case224.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case66.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case372.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case44.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case59.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case24.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case227.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case3.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case76.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case301.json (deflated 71%)\n",
      "  adding: kaggle/working/rrl/case359.json (deflated 80%)\n",
      "  adding: kaggle/working/rrl/case194.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case200.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case6.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case267.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case293.json (deflated 78%)\n",
      "  adding: kaggle/working/rrl/case13.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case171.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case18.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case350.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case398.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case62.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case179.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case369.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case181.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case304.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case178.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case387.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case305.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case158.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case123.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case204.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case34.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case32.json (deflated 76%)\n",
      "  adding: kaggle/working/rrl/case312.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case1.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case47.json (deflated 70%)\n",
      "  adding: kaggle/working/rrl/case392.json (deflated 79%)\n",
      "  adding: kaggle/working/rrl/case244.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case154.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case64.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case373.json (deflated 75%)\n",
      "  adding: kaggle/working/rrl/case100.json (deflated 77%)\n",
      "  adding: kaggle/working/rrl/case37.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case330.json (deflated 79%)\n",
      "  adding: kaggle/working/rrl/case221.json (deflated 72%)\n",
      "  adding: kaggle/working/rrl/case209.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case51.json (deflated 74%)\n",
      "  adding: kaggle/working/rrl/case239.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case54.json (deflated 81%)\n",
      "  adding: kaggle/working/rrl/case137.json (deflated 73%)\n",
      "  adding: kaggle/working/rrl/case16.json (deflated 73%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/rrl_final.zip /kaggle/working/rrl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:49:25.433596Z",
     "iopub.status.busy": "2025-07-15T10:49:25.433314Z",
     "iopub.status.idle": "2025-07-15T10:51:40.663559Z",
     "shell.execute_reply": "2025-07-15T10:51:40.662754Z",
     "shell.execute_reply.started": "2025-07-15T10:49:25.433579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate myenv\n",
    "\n",
    "# Run Python file using conda env interpreter\n",
    "python run_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:42:22.820205Z",
     "iopub.status.busy": "2025-07-15T10:42:22.819700Z",
     "iopub.status.idle": "2025-07-15T10:48:15.484605Z",
     "shell.execute_reply": "2025-07-15T10:48:15.483806Z",
     "shell.execute_reply.started": "2025-07-15T10:42:22.820171Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "# Activate conda env\n",
    "source /usr/local/etc/profile.d/conda.sh\n",
    "conda activate myenv\n",
    "\n",
    "# Install necessary libraries (exact versions)\n",
    "conda install -y cudatoolkit=11.3\n",
    "pip install -q opennyai \"numpy<2\" cupy-cuda113 \"spacy[cuda113]\"\n",
    "\n",
    "# Run your logic\n",
    "PYTHONUNBUFFERED=1 python - <<EOF\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from opennyai import Pipeline\n",
    "from opennyai.utils import Data\n",
    "import sys\n",
    "\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "\n",
    "sys.stdout.reconfigure(line_buffering=True)\n",
    "\n",
    "\n",
    "# Paths\n",
    "input_dir = \"/kaggle/working/DHARA/cases\"\n",
    "output_dir = \"/kaggle/working/DHARA/rrl\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Collect and read all .txt case files\n",
    "file_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\n",
    "texts = []\n",
    "doc_ids = []\n",
    "\n",
    "for path in file_paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        texts.append(f.read())\n",
    "        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n",
    "\n",
    "# 2. Wrap with OpenNyAI-compatible Data object\n",
    "data = Data(texts, use_gpu=True)\n",
    "\n",
    "# 3. Init pipeline (use GPU if available)\n",
    "pipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n",
    "\n",
    "# 4. Run RRL\n",
    "results = pipe(data)\n",
    "\n",
    "# 5. Save each result as individual JSON file\n",
    "for doc_id, result in zip(doc_ids, results):\n",
    "    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Inference complete. {len(results)} files saved to {output_dir}\")\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:33:26.458414Z",
     "iopub.status.busy": "2025-07-15T10:33:26.457808Z",
     "iopub.status.idle": "2025-07-15T10:33:28.343709Z",
     "shell.execute_reply": "2025-07-15T10:33:28.342903Z",
     "shell.execute_reply.started": "2025-07-15T10:33:26.458354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DHARA'...\n",
      "remote: Enumerating objects: 810, done.\u001b[K\n",
      "remote: Counting objects: 100% (810/810), done.\u001b[K\n",
      "remote: Compressing objects: 100% (800/800), done.\u001b[K\n",
      "remote: Total 810 (delta 9), reused 809 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (810/810), 11.82 MiB | 13.34 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AnshSinghal/DHARA.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T09:14:37.397761Z",
     "iopub.status.busy": "2025-07-15T09:14:37.397495Z",
     "iopub.status.idle": "2025-07-15T09:16:23.649645Z",
     "shell.execute_reply": "2025-07-15T09:16:23.648870Z",
     "shell.execute_reply.started": "2025-07-15T09:14:37.397740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
      "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2025-06-26, rust version 1.88.0 (6b00bc388 2025-06-23)\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
      " 20.1 MiB /  20.1 MiB (100 %)   6.7 MiB/s in  3s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
      " 29.5 MiB /  29.5 MiB (100 %)  11.0 MiB/s in  2s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
      " 76.3 MiB /  76.3 MiB (100 %)  11.8 MiB/s in  6s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
      "\n",
      "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.88.0 (6b00bc388 2025-06-23)\n",
      "\n",
      "\u001b[0m\u001b[1m\n",
      "Rust is installed now. Great!\n",
      "\u001b[0m\n",
      "To get started you may need to restart your current shell.\n",
      "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
      "Cargo's bin directory ($HOME/.cargo/bin).\n",
      "\n",
      "To configure your current shell, you need to source\n",
      "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
      "\n",
      "This is usually done by running one of the following (note the leading DOT):\n",
      ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
      "source \"$HOME/.cargo/env.fish\"  # For fish\n",
      "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "\u001b[2K\u001b[1A\u001b[2mcpython-3.10.16-linux-x86_64-gnu\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 19.79 MiB/19.81 MiB                                                                             \u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[2mInstalled \u001b[1mPython 3.10.16\u001b[0m \u001b[2min 1.35s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcpython-3.10.16-linux-x86_64-gnu\u001b[0m\n",
      "Pinned `\u001b[36m.python-version\u001b[39m` to `\u001b[32m3.10\u001b[39m`\n",
      "Initialized project `\u001b[36mworking\u001b[39m`\n",
      "--- Script Output ---\n",
      "Python executable: /kaggle/working/.venv/bin/python3\n",
      "CUDA_HOME: /usr/local/cuda\n",
      "LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib\n",
      "CuPy version: 13.5.1\n",
      "CUDA version from CuPy: 12090\n",
      "Available GPUs: 1\n",
      "spaCy version: 3.2.6\n",
      "spaCy GPU support: False\n",
      "❌ GPU support for spaCy is still not available.\n",
      "\n",
      "\n",
      "--- Alternative Solution 2: Install from source ---\n",
      "If the first solution doesn't work, try this alternative:\n",
      "\n",
      "# Alternative approach - install cupy from source with specific CUDA version\n",
      "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv remove cupy-cuda12x\n",
      "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add -q --no-cache cupy\n",
      "\n",
      "# Or try with pip inside uv environment\n",
      "!uv run pip install --no-cache-dir --force-reinstall cupy-cuda12x\n",
      "\n",
      "# Test again\n",
      "python_test = '''\n",
      "import os\n",
      "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
      "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
      "\n",
      "import spacy\n",
      "import cupy\n",
      "\n",
      "is_gpu_available = spacy.prefer_gpu()\n",
      "print(f\"spaCy GPU support: {is_gpu_available}\")\n",
      "\n",
      "if is_gpu_available:\n",
      "    gpu_id = cupy.cuda.runtime.getDevice()\n",
      "    print(f\"✅ spaCy is now using GPU ID: {gpu_id}\")\n",
      "else:\n",
      "    print(\"❌ GPU support for spaCy is still not available.\")\n",
      "'''\n",
      "\n",
      "!uv run python -c \"$python_test\"\n",
      "\n",
      "\n",
      "--- Solution 3: Manual CUDA environment setup ---\n",
      "Try this more comprehensive solution:\n",
      "\n",
      "# Remove existing installations and reinstall with proper CUDA setup\n",
      "!uv remove spacy cupy-cuda12x\n",
      "!uv sync\n",
      "\n",
      "# Set up CUDA environment more explicitly\n",
      "import os\n",
      "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
      "os.environ['CUDA_ROOT'] = '/usr/local/cuda'\n",
      "os.environ['CUDA_PATH'] = '/usr/local/cuda'\n",
      "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib64'\n",
      "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
      "\n",
      "# Install with explicit CUDA version\n",
      "!CUDA_HOME=/usr/local/cuda CUDA_ROOT=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add cupy==12.3.0\n",
      "\n",
      "# Install spacy after cupy is properly installed\n",
      "!CUDA_HOME=/usr/local/cuda CUDA_ROOT=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add \"spacy[cuda12x]\"\n",
      "\n",
      "# Test with proper environment\n",
      "test_script = '''\n",
      "import os\n",
      "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
      "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
      "\n",
      "try:\n",
      "    import cupy as cp\n",
      "    print(\"CuPy CUDA version:\", cp.cuda.runtime.runtimeGetVersion())\n",
      "    \n",
      "    # Test basic GPU operation\n",
      "    a = cp.array([1, 2, 3, 4, 5])\n",
      "    print(\"CuPy GPU test successful:\", a.sum())\n",
      "    \n",
      "    import spacy\n",
      "    \n",
      "    # This should now work\n",
      "    gpu_available = spacy.prefer_gpu()\n",
      "    print(f\"spaCy GPU available: {gpu_available}\")\n",
      "    \n",
      "    if gpu_available:\n",
      "        print(\"✅ Success! spaCy is using GPU\")\n",
      "    else:\n",
      "        print(\"❌ spaCy still not using GPU\")\n",
      "        \n",
      "except Exception as e:\n",
      "    print(\"Error:\", e)\n",
      "    import traceback\n",
      "    traceback.print_exc()\n",
      "'''\n",
      "\n",
      "!uv run python -c \"$test_script\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: Force install cupy-cuda12x with specific CUDA path\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ['PATH'] += \":/root/.cargo/bin\"\n",
    "\n",
    "# Set CUDA environment variables before installing packages\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['CUDA_ROOT'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:/usr/local/cuda/lib'\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "\n",
    "# Configure ldconfig\n",
    "!echo \"/usr/local/cuda/lib64\" >> /etc/ld.so.conf.d/cuda.conf\n",
    "!ldconfig\n",
    "\n",
    "# Set up uv with explicit CUDA environment\n",
    "!uv python install 3.10\n",
    "!uv python pin 3.10\n",
    "!uv init --python 3.10\n",
    "\n",
    "# Install packages with CUDA environment variables\n",
    "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add -q opennyai \"numpy<2\"\n",
    "\n",
    "# Install cupy first with explicit CUDA version\n",
    "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add -q \"cupy-cuda12x\"\n",
    "\n",
    "# Then install spacy with CUDA support\n",
    "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add -q \"spacy[cuda12x]\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Test script with environment variables\n",
    "python_code = \"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set CUDA environment variables in the Python process\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:/usr/local/cuda/lib'\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "\n",
    "# Print environment info\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"CUDA_HOME:\", os.environ.get('CUDA_HOME', 'Not set'))\n",
    "print(\"LD_LIBRARY_PATH:\", os.environ.get('LD_LIBRARY_PATH', 'Not set'))\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "    print(\"CuPy version:\", cupy.__version__)\n",
    "    print(\"CUDA version from CuPy:\", cupy.cuda.runtime.runtimeGetVersion())\n",
    "    print(\"Available GPUs:\", cupy.cuda.runtime.getDeviceCount())\n",
    "except Exception as e:\n",
    "    print(\"CuPy error:\", e)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"spaCy version:\", spacy.__version__)\n",
    "    \n",
    "    # Try to enable GPU\n",
    "    is_gpu_available = spacy.prefer_gpu()\n",
    "    print(f\"spaCy GPU support: {is_gpu_available}\")\n",
    "    \n",
    "    if is_gpu_available:\n",
    "        gpu_id = cupy.cuda.runtime.getDevice()\n",
    "        print(f\"✅ spaCy is now using GPU ID: {gpu_id}\")\n",
    "    else:\n",
    "        print(\"❌ GPU support for spaCy is still not available.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"spaCy error:\", e)\n",
    "\"\"\"\n",
    "\n",
    "# Run with explicit environment\n",
    "env = os.environ.copy()\n",
    "env.update({\n",
    "    'CUDA_HOME': '/usr/local/cuda',\n",
    "    'LD_LIBRARY_PATH': '/usr/local/cuda/lib64:/usr/local/cuda/lib',\n",
    "    'PATH': env['PATH'] + ':/usr/local/cuda/bin'\n",
    "})\n",
    "\n",
    "completed_process = subprocess.run(\n",
    "    [\"uv\", \"run\", \"python\", \"-c\", python_code], \n",
    "    capture_output=True, \n",
    "    text=True, \n",
    "    env=env\n",
    ")\n",
    "\n",
    "print(\"--- Script Output ---\")\n",
    "print(completed_process.stdout)\n",
    "if completed_process.stderr:\n",
    "    print(\"--- Errors ---\")\n",
    "    print(completed_process.stderr)\n",
    "\n",
    "\n",
    "# Alternative Solution 2: If above doesn't work, try installing from source\n",
    "print(\"\\n--- Alternative Solution 2: Install from source ---\")\n",
    "\n",
    "alternative_code = \"\"\"\n",
    "# Alternative approach - install cupy from source with specific CUDA version\n",
    "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv remove cupy-cuda12x\n",
    "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add -q --no-cache cupy\n",
    "\n",
    "# Or try with pip inside uv environment\n",
    "!uv run pip install --no-cache-dir --force-reinstall cupy-cuda12x\n",
    "\n",
    "# Test again\n",
    "python_test = '''\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
    "\n",
    "import spacy\n",
    "import cupy\n",
    "\n",
    "is_gpu_available = spacy.prefer_gpu()\n",
    "print(f\"spaCy GPU support: {is_gpu_available}\")\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_id = cupy.cuda.runtime.getDevice()\n",
    "    print(f\"✅ spaCy is now using GPU ID: {gpu_id}\")\n",
    "else:\n",
    "    print(\"❌ GPU support for spaCy is still not available.\")\n",
    "'''\n",
    "\n",
    "!uv run python -c \"$python_test\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"If the first solution doesn't work, try this alternative:\")\n",
    "print(alternative_code)\n",
    "\n",
    "\n",
    "# Solution 3: Manual cupy installation with specific CUDA version\n",
    "print(\"\\n--- Solution 3: Manual CUDA environment setup ---\")\n",
    "\n",
    "solution3_code = \"\"\"\n",
    "# Remove existing installations and reinstall with proper CUDA setup\n",
    "!uv remove spacy cupy-cuda12x\n",
    "!uv sync\n",
    "\n",
    "# Set up CUDA environment more explicitly\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['CUDA_ROOT'] = '/usr/local/cuda'\n",
    "os.environ['CUDA_PATH'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib64'\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "\n",
    "# Install with explicit CUDA version\n",
    "!CUDA_HOME=/usr/local/cuda CUDA_ROOT=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add cupy==12.3.0\n",
    "\n",
    "# Install spacy after cupy is properly installed\n",
    "!CUDA_HOME=/usr/local/cuda CUDA_ROOT=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add \"spacy[cuda12x]\"\n",
    "\n",
    "# Test with proper environment\n",
    "test_script = '''\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(\"CuPy CUDA version:\", cp.cuda.runtime.runtimeGetVersion())\n",
    "    \n",
    "    # Test basic GPU operation\n",
    "    a = cp.array([1, 2, 3, 4, 5])\n",
    "    print(\"CuPy GPU test successful:\", a.sum())\n",
    "    \n",
    "    import spacy\n",
    "    \n",
    "    # This should now work\n",
    "    gpu_available = spacy.prefer_gpu()\n",
    "    print(f\"spaCy GPU available: {gpu_available}\")\n",
    "    \n",
    "    if gpu_available:\n",
    "        print(\"✅ Success! spaCy is using GPU\")\n",
    "    else:\n",
    "        print(\"❌ spaCy still not using GPU\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "'''\n",
    "\n",
    "!uv run python -c \"$test_script\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"Try this more comprehensive solution:\")\n",
    "print(solution3_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T09:21:21.312853Z",
     "iopub.status.busy": "2025-07-15T09:21:21.312344Z",
     "iopub.status.idle": "2025-07-15T09:21:45.681180Z",
     "shell.execute_reply": "2025-07-15T09:21:45.680287Z",
     "shell.execute_reply.started": "2025-07-15T09:21:21.312826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m94 packages\u001b[0m \u001b[2min 65ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `spacy==3.2.6` does not have an extra named `cuda12x`\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 115ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcupy-cuda12x\u001b[0m\u001b[2m==13.5.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfastrlock\u001b[0m\u001b[2m==0.8.3\u001b[0m\n",
      "  \u001b[31m×\u001b[0m Failed to build `cupy==13.5.1`\n",
      "\u001b[31m  ├─▶ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ╰─▶ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
      "\u001b[31m      \u001b[0mClearing directory:\n",
      "\u001b[31m      \u001b[0m/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/cupy/.data\n",
      "\u001b[31m      \u001b[0mGenerating CUPY_CACHE_KEY from header files...\n",
      "\u001b[31m      \u001b[0mCUPY_CACHE_KEY (1729 files matching\n",
      "\u001b[31m      \u001b[0m/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/cupy/_core/include/**):\n",
      "\u001b[31m      \u001b[0m689d0ce84be1f21385336456dfa877e7e48b2366\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: cuda --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: cusolver --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: cudnn --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: nccl --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: nvtx --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: cutensor --------\n",
      "\u001b[31m      \u001b[0mcommand '/usr/bin/c++' failed with exit code 1\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: cub --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: jitify --------\n",
      "\u001b[31m      \u001b[0mCannot build a stub file.\n",
      "\u001b[31m      \u001b[0mOriginal error: command '/usr/bin/c++' failed with exit code 1\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: random --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: thrust --------\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: cusparselt --------\n",
      "\u001b[31m      \u001b[0mcommand '/usr/bin/c++' failed with exit code 1\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: dlpack --------\n",
      "\n",
      "\u001b[31m      \u001b[0m************************************************************\n",
      "\u001b[31m      \u001b[0m* CuPy Configuration Summary                               *\n",
      "\u001b[31m      \u001b[0m************************************************************\n",
      "\n",
      "\u001b[31m      \u001b[0mBuild Environment:\n",
      "\u001b[31m      \u001b[0m  Include directories:\n",
      "\u001b[31m      \u001b[0m['/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/cupy/_core/include/cupy/_cccl/libcudacxx',\n",
      "\u001b[31m      \u001b[0m'/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/cupy/_core/include/cupy/_cccl/thrust',\n",
      "\u001b[31m      \u001b[0m'/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/cupy/_core/include/cupy/_cccl/cub',\n",
      "\u001b[31m      \u001b[0m'/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/cupy/_core/include',\n",
      "\u001b[31m      \u001b[0m'/usr/local/cuda/include']\n",
      "\u001b[31m      \u001b[0m  Library directories: ['/usr/local/cuda/lib64']\n",
      "\u001b[31m      \u001b[0m  nvcc command       : ['/usr/local/cuda/bin/nvcc']\n",
      "\u001b[31m      \u001b[0m  hipcc command      : (not found)\n",
      "\n",
      "\u001b[31m      \u001b[0mEnvironment Variables:\n",
      "\u001b[31m      \u001b[0m  CFLAGS          : (none)\n",
      "\u001b[31m      \u001b[0m  LDFLAGS         : (none)\n",
      "\u001b[31m      \u001b[0m  LIBRARY_PATH    : /usr/local/cuda/lib64/stubs\n",
      "\u001b[31m      \u001b[0m  CUDA_PATH       : (none)\n",
      "\u001b[31m      \u001b[0m  NVCC            : (none)\n",
      "\u001b[31m      \u001b[0m  HIPCC           : (none)\n",
      "\u001b[31m      \u001b[0m  ROCM_HOME       : (none)\n",
      "\n",
      "\u001b[31m      \u001b[0mModules:\n",
      "\u001b[31m      \u001b[0m  cuda      : Yes (version 12050)\n",
      "\u001b[31m      \u001b[0m  cusolver  : Yes\n",
      "\u001b[31m      \u001b[0m  cudnn     : Yes (version 90201)\n",
      "\u001b[31m      \u001b[0m  nccl      : Yes (version 22203)\n",
      "\u001b[31m      \u001b[0m  nvtx      : Yes\n",
      "\u001b[31m      \u001b[0m  cutensor  : No\n",
      "\u001b[31m      \u001b[0m    -> Include files not found: ['cutensor.h']\n",
      "\u001b[31m      \u001b[0m    -> Check your CFLAGS environment variable.\n",
      "\u001b[31m      \u001b[0m  cub       : Yes (version 200800)\n",
      "\u001b[31m      \u001b[0m  jitify    : No\n",
      "\u001b[31m      \u001b[0m    -> Cannot link libraries: ['cuda', 'nvrtc', 'pthread', 'rt', 'dl']\n",
      "\u001b[31m      \u001b[0m    -> Check your LDFLAGS environment variable.\n",
      "\u001b[31m      \u001b[0m  random    : Yes\n",
      "\u001b[31m      \u001b[0m  thrust    : Yes (version 200800)\n",
      "\u001b[31m      \u001b[0m  cusparselt: No\n",
      "\u001b[31m      \u001b[0m    -> Include files not found: ['cusparseLt.h']\n",
      "\u001b[31m      \u001b[0m    -> Check your CFLAGS environment variable.\n",
      "\u001b[31m      \u001b[0m  dlpack    : Yes\n",
      "\n",
      "\u001b[31m      \u001b[0mWARNING: Some modules could not be configured.\n",
      "\u001b[31m      \u001b[0mCuPy will be installed without these modules.\n",
      "\u001b[31m      \u001b[0mPlease refer to the Installation Guide for details:\n",
      "\u001b[31m      \u001b[0mhttps://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "\u001b[31m      \u001b[0m************************************************************\n",
      "\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0m/tmp/tmpgeg85o3w/a.cpp:1:10: fatal error: cutensor.h: No such file or\n",
      "\u001b[31m      \u001b[0mdirectory\n",
      "\u001b[31m      \u001b[0m    1 | #include <cutensor.h>\n",
      "\u001b[31m      \u001b[0m      |          ^~~~~~~~~~~~\n",
      "\u001b[31m      \u001b[0mcompilation terminated.\n",
      "\u001b[31m      \u001b[0m/usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[31m      \u001b[0mcollect2: error: ld returned 1 exit status\n",
      "\u001b[31m      \u001b[0m/tmp/tmp2epr8b6h/a.cpp:1:10: fatal error: cusparseLt.h: No such file\n",
      "\u001b[31m      \u001b[0mor directory\n",
      "\u001b[31m      \u001b[0m    1 | #include <cusparseLt.h>\n",
      "\u001b[31m      \u001b[0m      |          ^~~~~~~~~~~~~~\n",
      "\u001b[31m      \u001b[0mcompilation terminated.\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 11, in <module>\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/tmp/.tmpCqfom2/builds-v0/.tmpap70i8/lib/python3.10/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 432, in build_wheel\n",
      "\u001b[31m      \u001b[0m    return _build(['bdist_wheel'])\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/tmp/.tmpCqfom2/builds-v0/.tmpap70i8/lib/python3.10/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 423, in _build\n",
      "\u001b[31m      \u001b[0m    return self._build_with_temp_dir(\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/tmp/.tmpCqfom2/builds-v0/.tmpap70i8/lib/python3.10/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 404, in _build_with_temp_dir\n",
      "\u001b[31m      \u001b[0m    self.run_setup()\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/tmp/.tmpCqfom2/builds-v0/.tmpap70i8/lib/python3.10/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 317, in run_setup\n",
      "\u001b[31m      \u001b[0m    exec(code, locals())\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 59, in <module>\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/install/cupy_builder/cupy_setup_build.py\",\n",
      "\u001b[31m      \u001b[0mline 562, in get_ext_modules\n",
      "\u001b[31m      \u001b[0m    extensions = make_extensions(ctx, compiler, use_cython)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/tmp/.tmpCqfom2/sdists-v9/pypi/cupy/13.5.1/jFHNDpFg7OJ531j29_2zp/src/install/cupy_builder/cupy_setup_build.py\",\n",
      "\u001b[31m      \u001b[0mline 403, in make_extensions\n",
      "\u001b[31m      \u001b[0m    raise Exception('Your CUDA environment is invalid. '\n",
      "\u001b[31m      \u001b[0mException: Your CUDA environment is invalid. Please check above error\n",
      "\u001b[31m      \u001b[0mlog.\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
      "\u001b[31m      \u001b[0menvironment.\n",
      "\u001b[36m  help: \u001b[0mIf you want to add the package regardless of the failed resolution,\n",
      "        provide the `\u001b[32m--frozen\u001b[39m` flag to skip locking and syncing.\n",
      "Collecting cupy-cuda12x\n",
      "  Downloading cupy_cuda12x-13.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting numpy<2.6,>=1.22 (from cupy-cuda12x)\n",
      "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastrlock>=0.5 (from cupy-cuda12x)\n",
      "  Downloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading cupy_cuda12x-13.5.1-cp311-cp311-manylinux2014_x86_64.whl (113.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 MB\u001b[0m \u001b[31m304.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m232.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m183.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fastrlock, numpy, cupy-cuda12x\n",
      "  Attempting uninstall: fastrlock\n",
      "    Found existing installation: fastrlock 0.8.3\n",
      "    Uninstalling fastrlock-0.8.3:\n",
      "      Successfully uninstalled fastrlock-0.8.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: cupy-cuda12x\n",
      "    Found existing installation: cupy-cuda12x 13.4.1\n",
      "    Uninstalling cupy-cuda12x-13.4.1:\n",
      "      Successfully uninstalled cupy-cuda12x-13.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.1 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cupy-cuda12x-13.5.1 fastrlock-0.8.3 numpy-2.3.1\n",
      "  File \"<string>\", line 10\n",
      "    print(fspaCy\n",
      "         ^\n",
      "SyntaxError: '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv remove cupy-cuda12x\n",
    "!CUDA_HOME=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add -q --no-cache cupy\n",
    "\n",
    "# Or try with pip inside uv environment\n",
    "!uv run pip install --no-cache-dir --force-reinstall cupy-cuda12x\n",
    "\n",
    "# Test again\n",
    "python_test = '''\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
    "\n",
    "import spacy\n",
    "import cupy\n",
    "\n",
    "is_gpu_available = spacy.prefer_gpu()\n",
    "print(f\"spaCy GPU support: {is_gpu_available}\")\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_id = cupy.cuda.runtime.getDevice()\n",
    "    print(f\"✅ spaCy is now using GPU ID: {gpu_id}\")\n",
    "else:\n",
    "    print(\"❌ GPU support for spaCy is still not available.\")\n",
    "'''\n",
    "\n",
    "!uv run python -c \"$python_test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T09:22:18.340265Z",
     "iopub.status.busy": "2025-07-15T09:22:18.339481Z",
     "iopub.status.idle": "2025-07-15T09:22:20.967566Z",
     "shell.execute_reply": "2025-07-15T09:22:20.966862Z",
     "shell.execute_reply.started": "2025-07-15T09:22:18.340226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: The dependency `cupy-cuda12x` could not be found in `project.dependencies`\n",
      "\u001b[2mResolved \u001b[1m94 packages\u001b[0m \u001b[2min 0.80ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m90 packages\u001b[0m \u001b[2min 0.03ms\u001b[0m\u001b[0m\n",
      "\u001b[2K  \u001b[31m×\u001b[0m Failed to build `cupy==12.3.0`                                                  \u001b[0m\n",
      "\u001b[31m  ├─▶ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ╰─▶ \u001b[0mCall to\n",
      "\u001b[31m      \u001b[0m`setuptools.build_meta:__legacy__.prepare_metadata_for_build_wheel`\n",
      "\u001b[31m      \u001b[0mfailed (exit status: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
      "\u001b[31m      \u001b[0mClearing directory:\n",
      "\u001b[31m      \u001b[0m/root/.cache/uv/sdists-v9/pypi/cupy/12.3.0/GRf_ch1elRNviAZb-btia/src/cupy/.data\n",
      "\n",
      "\u001b[31m      \u001b[0m-------- Configuring Module: cuda --------\n",
      "\u001b[31m      \u001b[0mCannot build a stub file.\n",
      "\u001b[31m      \u001b[0mOriginal error: command '/usr/bin/c++' failed with exit code 1\n",
      "\n",
      "\u001b[31m      \u001b[0m************************************************************\n",
      "\u001b[31m      \u001b[0m* CuPy Configuration Summary                               *\n",
      "\u001b[31m      \u001b[0m************************************************************\n",
      "\n",
      "\u001b[31m      \u001b[0mBuild Environment:\n",
      "\u001b[31m      \u001b[0m  Include directories: ['/usr/local/cuda/include/cub',\n",
      "\u001b[31m      \u001b[0m'/root/.cache/uv/sdists-v9/pypi/cupy/12.3.0/GRf_ch1elRNviAZb-btia/src/cupy/_core/include',\n",
      "\u001b[31m      \u001b[0m'/usr/local/cuda/include']\n",
      "\u001b[31m      \u001b[0m  Library directories: ['/usr/local/cuda/lib64']\n",
      "\u001b[31m      \u001b[0m  nvcc command       : ['/usr/local/cuda/bin/nvcc']\n",
      "\u001b[31m      \u001b[0m  hipcc command      : (not found)\n",
      "\n",
      "\u001b[31m      \u001b[0mEnvironment Variables:\n",
      "\u001b[31m      \u001b[0m  CFLAGS          : (none)\n",
      "\u001b[31m      \u001b[0m  LDFLAGS         : (none)\n",
      "\u001b[31m      \u001b[0m  LIBRARY_PATH    : /usr/local/cuda/lib64/stubs\n",
      "\u001b[31m      \u001b[0m  CUDA_PATH       : /usr/local/cuda\n",
      "\u001b[31m      \u001b[0m  NVCC            : (none)\n",
      "\u001b[31m      \u001b[0m  HIPCC           : (none)\n",
      "\u001b[31m      \u001b[0m  ROCM_HOME       : (none)\n",
      "\n",
      "\u001b[31m      \u001b[0mModules:\n",
      "\u001b[31m      \u001b[0m  cuda      : No\n",
      "\u001b[31m      \u001b[0m    -> Cannot link libraries: ['cuda', 'cudart', 'cublas', 'cufft',\n",
      "\u001b[31m      \u001b[0m'curand', 'cusparse', 'nvrtc']\n",
      "\u001b[31m      \u001b[0m    -> Check your LDFLAGS environment variable.\n",
      "\n",
      "\u001b[31m      \u001b[0mERROR: CUDA could not be found on your system.\n",
      "\n",
      "\u001b[31m      \u001b[0mHINT: You are trying to build CuPy from source, which is NOT recommended\n",
      "\u001b[31m      \u001b[0mfor general use.\n",
      "\u001b[31m      \u001b[0m      Please consider using binary packages instead.\n",
      "\n",
      "\u001b[31m      \u001b[0mPlease refer to the Installation Guide for details:\n",
      "\u001b[31m      \u001b[0mhttps://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "\u001b[31m      \u001b[0m************************************************************\n",
      "\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0m/usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[31m      \u001b[0mcollect2: error: ld returned 1 exit status\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 14, in <module>\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpbuWSP3/lib/python3.10/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 374, in prepare_metadata_for_build_wheel\n",
      "\u001b[31m      \u001b[0m    self.run_setup()\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpbuWSP3/lib/python3.10/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 512, in run_setup\n",
      "\u001b[31m      \u001b[0m    super().run_setup(setup_script=setup_script)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpbuWSP3/lib/python3.10/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 317, in run_setup\n",
      "\u001b[31m      \u001b[0m    exec(code, locals())\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 88, in <module>\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/sdists-v9/pypi/cupy/12.3.0/GRf_ch1elRNviAZb-btia/src/install/cupy_builder/cupy_setup_build.py\",\n",
      "\u001b[31m      \u001b[0mline 449, in get_ext_modules\n",
      "\u001b[31m      \u001b[0m    extensions = make_extensions(ctx, compiler, use_cython)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/sdists-v9/pypi/cupy/12.3.0/GRf_ch1elRNviAZb-btia/src/install/cupy_builder/cupy_setup_build.py\",\n",
      "\u001b[31m      \u001b[0mline 305, in make_extensions\n",
      "\u001b[31m      \u001b[0m    raise Exception('Your CUDA environment is invalid. '\n",
      "\u001b[31m      \u001b[0mException: Your CUDA environment is invalid. Please check above error\n",
      "\u001b[31m      \u001b[0mlog.\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
      "\u001b[31m      \u001b[0menvironment.\n",
      "\u001b[36m  help: \u001b[0mIf you want to add the package regardless of the failed resolution,\n",
      "        provide the `\u001b[32m--frozen\u001b[39m` flag to skip locking and syncing.\n",
      "\u001b[2mResolved \u001b[1m94 packages\u001b[0m \u001b[2min 0.74ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m90 packages\u001b[0m \u001b[2min 0.03ms\u001b[0m\u001b[0m\n",
      "  File \"<string>\", line 8\n",
      "    print(CuPy\n",
      "         ^\n",
      "SyntaxError: '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "!uv remove spacy cupy-cuda12x\n",
    "!uv sync\n",
    "\n",
    "# Set up CUDA environment more explicitly\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['CUDA_ROOT'] = '/usr/local/cuda'\n",
    "os.environ['CUDA_PATH'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib64'\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "\n",
    "# Install with explicit CUDA version\n",
    "!CUDA_HOME=/usr/local/cuda CUDA_ROOT=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add cupy==12.3.0\n",
    "\n",
    "# Install spacy after cupy is properly installed\n",
    "!CUDA_HOME=/usr/local/cuda CUDA_ROOT=/usr/local/cuda LD_LIBRARY_PATH=/usr/local/cuda/lib64 uv add \"spacy[cuda12x]\"\n",
    "\n",
    "# Test with proper environment\n",
    "test_script = '''\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(\"CuPy CUDA version:\", cp.cuda.runtime.runtimeGetVersion())\n",
    "    \n",
    "    # Test basic GPU operation\n",
    "    a = cp.array([1, 2, 3, 4, 5])\n",
    "    print(\"CuPy GPU test successful:\", a.sum())\n",
    "    \n",
    "    import spacy\n",
    "    \n",
    "    # This should now work\n",
    "    gpu_available = spacy.prefer_gpu()\n",
    "    print(f\"spaCy GPU available: {gpu_available}\")\n",
    "    \n",
    "    if gpu_available:\n",
    "        print(\"✅ Success! spaCy is using GPU\")\n",
    "    else:\n",
    "        print(\"❌ spaCy still not using GPU\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "'''\n",
    "\n",
    "!uv run python -c \"$test_script\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T09:24:02.692418Z",
     "iopub.status.busy": "2025-07-15T09:24:02.692101Z",
     "iopub.status.idle": "2025-07-15T09:24:02.810843Z",
     "shell.execute_reply": "2025-07-15T09:24:02.810179Z",
     "shell.execute_reply.started": "2025-07-15T09:24:02.692393Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: strace: command not found\n"
     ]
    }
   ],
   "source": [
    "!strace -e trace=open,openat -o /tmp/uv_spacy_trace.txt uv run python -c \"import spacy; spacy.require_gpu()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T09:25:33.748948Z",
     "iopub.status.busy": "2025-07-15T09:25:33.748681Z",
     "iopub.status.idle": "2025-07-15T09:25:33.756515Z",
     "shell.execute_reply": "2025-07-15T09:25:33.755802Z",
     "shell.execute_reply.started": "2025-07-15T09:25:33.748929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_LIB_PATH: /usr/local/cuda/lib64\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Get the path to nvcc\n",
    "nvcc_path = subprocess.check_output(['which', 'nvcc']).decode().strip()\n",
    "\n",
    "# Get CUDA lib path by trimming the path appropriately\n",
    "cuda_lib_path = os.path.join(os.path.dirname(os.path.dirname(nvcc_path)), 'lib64')\n",
    "\n",
    "print(\"CUDA_LIB_PATH:\", cuda_lib_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T09:26:42.396648Z",
     "iopub.status.busy": "2025-07-15T09:26:42.395845Z",
     "iopub.status.idle": "2025-07-15T09:26:42.554994Z",
     "shell.execute_reply": "2025-07-15T09:26:42.554362Z",
     "shell.execute_reply.started": "2025-07-15T09:26:42.396614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/.venv/bin/python3: can't open file '/kaggle/working/your_script.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# The core of the solution:\n",
    "# Use the previously discovered CUDA_LIB_PATH to set LD_LIBRARY_PATH\n",
    "# for the duration of the 'uv run' command.\n",
    "!LD_LIBRARY_PATH={CUDA_LIB_PATH} uv run python your_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# --- Step 1: Dynamically locate the CUDA library path ---\n",
    "# This is more robust than hardcoding the path.\n",
    "try:\n",
    "    nvcc_path = subprocess.check_output(\"which nvcc\", shell=True, text=True).strip()\n",
    "    cuda_root_path = os.path.dirname(os.path.dirname(nvcc_path))\n",
    "    CUDA_LIB_PATH = os.path.join(cuda_root_path, \"lib64\")\n",
    "    print(f\"SUCCESS: Discovered CUDA library path: {CUDA_LIB_PATH}\")\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    print(\"ERROR: nvcc not found. Cannot determine CUDA library path.\")\n",
    "    CUDA_LIB_PATH = None\n",
    "\n",
    "if CUDA_LIB_PATH:\n",
    "    # --- Step 2: Install dependencies with uv ---\n",
    "    print(\"\\n--- Installing dependencies with uv for Python 3.10 ---\")\n",
    "    install_command = (\n",
    "        'uv pip install \"spacy[cuda12x]==3.7.4\" \"cupy-cuda12x==12.3.0\" '\n",
    "        '\"torch==2.1.2\" \"torchaudio==2.1.2\" --python 3.10'\n",
    "    )\n",
    "    subprocess.run(install_command, shell=True, check=True)\n",
    "    print(\"--- Installation complete ---\")\n",
    "\n",
    "    # --- Step 3: Create a verification script ---\n",
    "    verify_script_content = \"\"\"\n",
    "import torch\n",
    "import spacy\n",
    "import cupy\n",
    "import os\n",
    "\n",
    "print(f\"--- Verification inside uv run environment ---\")\n",
    "print(f\"LD_LIBRARY_PATH: {os.environ.get('LD_LIBRARY_PATH', 'Not Set')}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verify PyTorch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torch CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Torch device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verify spaCy and CuPy\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "print(f\"CuPy version: {cupy.__version__}\")\n",
    "try:\n",
    "    # spacy.require_gpu() returns True if a GPU is available, otherwise raises an error.\n",
    "    spacy.require_gpu(0)\n",
    "    print(\"spacy.require_gpu(0) check PASSED.\")\n",
    "    # Further test by moving a small pipeline to the GPU\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    nlp.add_pipe(\"textcat\")\n",
    "    nlp.to_gpu(0)\n",
    "    print(\"spaCy model successfully moved to GPU.\")\n",
    "    nlp.to_cpu() # Clean up GPU memory\n",
    "    print(\"SUCCESS: spaCy GPU functionality is working correctly.\")\n",
    "except Exception as e:\n",
    "    print(f\"FAILURE: spaCy GPU check failed: {e}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\"\"\"\n",
    "    with open(\"verify.py\", \"w\") as f:\n",
    "        f.write(verify_script_content)\n",
    "    print(\"--- Verification script 'verify.py' created ---\")\n",
    "\n",
    "    # --- Step 4: Execute the script with the injected LD_LIBRARY_PATH ---\n",
    "    print(\"\\n--- Running verification script ---\")\n",
    "    run_command = f\"LD_LIBRARY_PATH={CUDA_LIB_PATH} uv run python verify.py\"\n",
    "    subprocess.run(run_command, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:49:09.879200Z",
     "iopub.status.busy": "2025-07-15T08:49:09.878963Z",
     "iopub.status.idle": "2025-07-15T08:51:12.714593Z",
     "shell.execute_reply": "2025-07-15T08:51:12.713162Z",
     "shell.execute_reply.started": "2025-07-15T08:49:09.879181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
      "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2025-06-26, rust version 1.88.0 (6b00bc388 2025-06-23)\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
      " 20.1 MiB /  20.1 MiB (100 %)  10.7 MiB/s in  1s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
      " 29.5 MiB /  29.5 MiB (100 %)  14.5 MiB/s in  2s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
      " 76.3 MiB /  76.3 MiB (100 %)  15.0 MiB/s in  5s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
      " 20.1 MiB /  20.1 MiB (100 %)   5.0 MiB/s in  3s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
      " 29.5 MiB /  29.5 MiB (100 %)  11.0 MiB/s in  2s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
      " 76.3 MiB /  76.3 MiB (100 %)  11.9 MiB/s in  6s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
      "\n",
      "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.88.0 (6b00bc388 2025-06-23)\n",
      "\n",
      "\u001b[0m\u001b[1m\n",
      "Rust is installed now. Great!\n",
      "\u001b[0m\n",
      "To get started you may need to restart your current shell.\n",
      "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
      "Cargo's bin directory ($HOME/.cargo/bin).\n",
      "\n",
      "To configure your current shell, you need to source\n",
      "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
      "\n",
      "This is usually done by running one of the following (note the leading DOT):\n",
      ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
      "source \"$HOME/.cargo/env.fish\"  # For fish\n",
      "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "\u001b[2K\u001b[1A\u001b[2mcpython-3.10.16-linux-x86_64-gnu\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 19.76 MiB/19.81 MiB                                                                             \u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[2mInstalled \u001b[1mPython 3.10.16\u001b[0m \u001b[2min 3.40s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcpython-3.10.16-linux-x86_64-gnu\u001b[0m\n",
      "Pinned `\u001b[36m.python-version\u001b[39m` to `\u001b[32m3.10\u001b[39m`\n",
      "Initialized project `\u001b[36mworking\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "# 1. Install Rust (for the 'tokenizers' dependency)\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ['PATH'] += \":/root/.cargo/bin\"\n",
    "\n",
    "# 2. Configure the system's dynamic linker to find CUDA libraries\n",
    "# This makes the libraries available globally for the session.\n",
    "!echo \"/usr/local/cuda/lib64\" >> /etc/ld.so.conf.d/cuda.conf\n",
    "!ldconfig\n",
    "\n",
    "# 3. Set up uv and install all packages in one command\n",
    "!uv python install 3.10\n",
    "!uv python pin 3.10\n",
    "!uv init --python 3.10\n",
    "!uv add -q opennyai \"spacy[cuda12x]\" \"numpy<2\" \"cupy-cuda12x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T09:00:17.091732Z",
     "iopub.status.busy": "2025-07-15T09:00:17.091475Z",
     "iopub.status.idle": "2025-07-15T09:00:20.048708Z",
     "shell.execute_reply": "2025-07-15T09:00:20.048096Z",
     "shell.execute_reply.started": "2025-07-15T09:00:17.091711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Script Output ---\n",
      "spaCy GPU support: False\n",
      "❌ GPU support for spaCy is still not available.\n",
      "\n",
      "--- Errors ---\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 15, in <module>\n",
      "AssertionError: spaCy GPU not available\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Because we used ldconfig, we no longer need to modify the environment here.\n",
    "# The system now knows where to find the CUDA libraries.\n",
    "python_code = \"\"\"\n",
    "import spacy\n",
    "import cupy\n",
    "\n",
    "# This should now work correctly\n",
    "is_gpu_available = spacy.prefer_gpu()\n",
    "print(f\"spaCy GPU support: {is_gpu_available}\")\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_id = cupy.cuda.runtime.getDevice()\n",
    "    print(f\"✅ spaCy is now using GPU ID: {gpu_id}\")\n",
    "else:\n",
    "    print(\"❌ GPU support for spaCy is still not available.\")\n",
    "\n",
    "assert is_gpu_available, \"spaCy GPU not available\"\n",
    "\"\"\"\n",
    "\n",
    "# Run the simplified verification script inside the uv environment\n",
    "completed_process = subprocess.run([\"uv\", \"run\", \"python\", \"-c\", python_code], capture_output=True, text=True)\n",
    "\n",
    "print(\"--- Script Output ---\")\n",
    "print(completed_process.stdout)\n",
    "if completed_process.stderr:\n",
    "    print(\"--- Errors ---\")\n",
    "    print(completed_process.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:57:15.849769Z",
     "iopub.status.busy": "2025-07-15T08:57:15.849364Z",
     "iopub.status.idle": "2025-07-15T08:57:15.975071Z",
     "shell.execute_reply": "2025-07-15T08:57:15.974446Z",
     "shell.execute_reply.started": "2025-07-15T08:57:15.849729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror:\u001b[0m unexpected argument '\u001b[33m--mount\u001b[0m' found\n",
      "\n",
      "  \u001b[32mtip:\u001b[0m a similar argument exists: '\u001b[32m--module\u001b[0m'\n",
      "\n",
      "\u001b[1m\u001b[32mUsage:\u001b[0m \u001b[1m\u001b[36muv run\u001b[0m \u001b[1m\u001b[36m--module\u001b[0m\n",
      "\n",
      "For more information, try '\u001b[1m\u001b[36m--help\u001b[0m'.\n"
     ]
    }
   ],
   "source": [
    "!uv run \\\n",
    "  --mount \"/usr/local/cuda:/usr/local/cuda\" \\\n",
    "  --mount \"/usr/local/cuda/lib64:/usr/local/cuda/lib64\" \\\n",
    "  python -c \"import spacy; print('GPU?', spacy.prefer_gpu())\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:58:57.107760Z",
     "iopub.status.busy": "2025-07-15T08:58:57.107125Z",
     "iopub.status.idle": "2025-07-15T08:59:04.765811Z",
     "shell.execute_reply": "2025-07-15T08:59:04.764912Z",
     "shell.execute_reply.started": "2025-07-15T08:58:57.107727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
      "spaCy GPU support: True\n",
      "✅ Using GPU ID: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EOF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1124164116.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spaCy still can’t see the GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mEOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'EOF' is not defined"
     ]
    }
   ],
   "source": [
    "!uv run python - <<EOF\n",
    "import spacy, cupy\n",
    "gpu_ok = spacy.prefer_gpu()\n",
    "print(\"spaCy GPU support:\", gpu_ok)\n",
    "if gpu_ok:\n",
    "    print(\"✅ Using GPU ID:\", cupy.cuda.runtime.getDevice())\n",
    "else:\n",
    "    raise RuntimeError(\"spaCy still can’t see the GPU\")\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:55:57.412474Z",
     "iopub.status.busy": "2025-07-15T08:55:57.411973Z",
     "iopub.status.idle": "2025-07-15T08:55:57.545556Z",
     "shell.execute_reply": "2025-07-15T08:55:57.544826Z",
     "shell.execute_reply.started": "2025-07-15T08:55:57.412450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T14:57:25.627496Z",
     "iopub.status.busy": "2025-07-14T14:57:25.627272Z",
     "iopub.status.idle": "2025-07-14T14:57:30.827445Z",
     "shell.execute_reply": "2025-07-14T14:57:30.826720Z",
     "shell.execute_reply.started": "2025-07-14T14:57:25.627477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-14T14:57:30.828697Z",
     "iopub.status.busy": "2025-07-14T14:57:30.828427Z",
     "iopub.status.idle": "2025-07-14T14:58:43.882726Z",
     "shell.execute_reply": "2025-07-14T14:58:43.880806Z",
     "shell.execute_reply.started": "2025-07-14T14:57:30.828664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:57:44.997778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752505065.208762      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752505065.273301      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c849657b6deb4fe88649c009251ea345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb362bdac294b33ad95041bb7bf7234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d679a1607d49e08407e26ba1be960e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b89e03ff1cb46c890006ba05325598e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/234696 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f2111534b54c858e27a866bc8f0aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb74bec98c34b6bb49a3bd0e65a2f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbad63f9cff143c0a3b56fee95f49cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/534M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at law-ai/InLegalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f7f45157d94768840cee7137fdc6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cad4c6361b4b20ade8bb83f506e804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/534M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeafc78089e44d4889261a0fbed86b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/3111294135.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Training setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./checkpoints\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset, DatasetDict\n",
    "# import pandas as pd\n",
    "# import evaluate\n",
    "# import numpy as np\n",
    "\n",
    "# # Load your CSV\n",
    "# df = pd.read_csv(\"/kaggle/input/legalseg/val.csv\", keep_default_na=False)\n",
    "\n",
    "# # Clean column names if needed\n",
    "# df.columns = [c.strip().lower() for c in df.columns]  # ensure 'text' and 'label'\n",
    "\n",
    "# # Map labels to IDs\n",
    "# label_list = [\n",
    "#     \"Facts\", \"Issue\", \"Arguments of Petitioner\", \"Arguments of Respondent\",\n",
    "#     \"Reasoning\", \"Decision\", \"None\"\n",
    "# ]\n",
    "# label2id = {label: i for i, label in enumerate(label_list)}\n",
    "# id2label = {i: label for label, i in label2id.items()}\n",
    "# df[\"label\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "# # Split the dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# # Convert to Hugging Face datasets\n",
    "# from datasets import Dataset\n",
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "# test_dataset = Dataset.from_pandas(test_df)\n",
    "# dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "# # Tokenize\n",
    "# model_name = \"law-ai/InLegalBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# def tokenize(example):\n",
    "#     return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# dataset = dataset.map(tokenize, batched=True)\n",
    "# dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "# dataset.set_format(\"torch\")\n",
    "\n",
    "# # Load model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=len(label_list),\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id\n",
    "# )\n",
    "\n",
    "# # Metrics\n",
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "# f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     preds = np.argmax(logits, axis=1)\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "#         \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "#     }\n",
    "\n",
    "# # Training setup\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./checkpoints\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=10,\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"f1_macro\"\n",
    "# )\n",
    "\n",
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"test\"],\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# # Train\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-14T14:58:43.883927Z",
     "iopub.status.idle": "2025-07-14T14:58:43.884323Z",
     "shell.execute_reply": "2025-07-14T14:58:43.884179Z",
     "shell.execute_reply.started": "2025-07-14T14:58:43.884167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/kaggle/input/legalseg/val.csv\", keep_default_na=False)\n",
    "# df.columns = [c.strip().lower() for c in df.columns]  # ensure 'text' and 'label'\n",
    "\n",
    "# # Map labels to IDs\n",
    "# label_list = [\n",
    "#     \"Facts\", \"Issue\", \"Arguments of Petitioner\", \"Arguments of Respondent\",\n",
    "#     \"Reasoning\", \"Decision\", \"None\"\n",
    "# ]\n",
    "# label2id = {label: i for i, label in enumerate(label_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-14T14:58:43.885162Z",
     "iopub.status.idle": "2025-07-14T14:58:43.885511Z",
     "shell.execute_reply": "2025-07-14T14:58:43.885393Z",
     "shell.execute_reply.started": "2025-07-14T14:58:43.885377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Temporary mapping without dropping NaNs\n",
    "# df[\"mapped_label\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "# # Show rows where mapping failed\n",
    "# invalid_rows = df[df[\"mapped_label\"].isna()]\n",
    "# print(\"Rows with unmapped labels:\\n\", invalid_rows[[\"label\"]].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-14T14:58:43.886327Z",
     "iopub.status.idle": "2025-07-14T14:58:43.886549Z",
     "shell.execute_reply": "2025-07-14T14:58:43.886456Z",
     "shell.execute_reply.started": "2025-07-14T14:58:43.886446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unmapped_labels = df[~df[\"label\"].isin(label2id.keys())][\"label\"].unique()\n",
    "# print(\"Unmapped labels:\", unmapped_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-14T14:58:43.887328Z",
     "iopub.status.idle": "2025-07-14T14:58:43.887546Z",
     "shell.execute_reply": "2025-07-14T14:58:43.887459Z",
     "shell.execute_reply.started": "2025-07-14T14:58:43.887450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/kaggle/input/legalseg/val.csv\", keep_default_na=False)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T02:32:59.773091Z",
     "iopub.status.busy": "2025-07-15T02:32:59.772851Z",
     "iopub.status.idle": "2025-07-15T02:33:08.623911Z",
     "shell.execute_reply": "2025-07-15T02:33:08.623130Z",
     "shell.execute_reply.started": "2025-07-15T02:32:59.773071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manshsinghal\u001b[0m (\u001b[33manshsinghal-bennett-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:48:51.870600Z",
     "iopub.status.busy": "2025-07-14T15:48:51.870195Z",
     "iopub.status.idle": "2025-07-14T15:51:37.381330Z",
     "shell.execute_reply": "2025-07-14T15:51:37.380407Z",
     "shell.execute_reply.started": "2025-07-14T15:48:51.870575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 15:49:07.056513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752508147.242484      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752508147.296870      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a65e3abbb04da98c0834523e1e9f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd8554d6ab647c8a163134fb6f52831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d84c70a88f142ca9211d0449958a1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2249c69295b34a46beac0adc9f5bc980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c10b01a5ea54f4f9cbfcd1c49d5c973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/534M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at law-ai/InLegalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212ca70e27a342209ffac99d6586a96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/610829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ccf5f0abba4392a19412dd963d50b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/534M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1966f342d4a1494585c716e943a91d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/434307258.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# --- 5. TrainingArguments ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results/inlegalbert-finetuned-legalseg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- 1. Load and Combine CSVs ---\n",
    "# Load the CSVs\n",
    "train_df = pd.read_csv('/kaggle/input/legalseg/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/legalseg/test.csv')\n",
    "val_df = pd.read_csv('/kaggle/input/legalseg/val.csv')\n",
    "\n",
    "# Drop unused column and clean up\n",
    "for df in [train_df, test_df, val_df]:\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(\"Index\", axis=1, inplace=True)\n",
    "    df.columns = ['text', 'label_str']\n",
    "\n",
    "# Combine train and test\n",
    "combined_train_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Create consistent label mapping from all data\n",
    "unique_labels = pd.concat([combined_train_df, val_df])['label_str'].unique()\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Map labels\n",
    "combined_train_df['label'] = combined_train_df['label_str'].map(label2id)\n",
    "val_df['label'] = val_df['label_str'].map(label2id)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(combined_train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# --- 2. Tokenizer and Model ---\n",
    "model_name = \"law-ai/InLegalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# --- 3. Tokenization ---\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# --- 4. Metric Computation for WandB logging ---\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# --- 5. TrainingArguments ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:53:49.093642Z",
     "iopub.status.busy": "2025-07-14T15:53:49.092946Z",
     "iopub.status.idle": "2025-07-14T17:37:18.721674Z",
     "shell.execute_reply": "2025-07-14T17:37:18.719905Z",
     "shell.execute_reply.started": "2025-07-14T15:53:49.093612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/1354232618.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250714_155349-41q5ldga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anshsinghal-bennett-university/huggingface/runs/41q5ldga' target=\"_blank\">inlegalbert-legalseg-finetune</a></strong> to <a href='https://wandb.ai/anshsinghal-bennett-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anshsinghal-bennett-university/huggingface' target=\"_blank\">https://wandb.ai/anshsinghal-bennett-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anshsinghal-bennett-university/huggingface/runs/41q5ldga' target=\"_blank\">https://wandb.ai/anshsinghal-bennett-university/huggingface/runs/41q5ldga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13649' max='57267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13649/57267 1:43:18 < 5:30:10, 2.20 it/s, Epoch 0.71/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1354232618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# --- 7. Train ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# --- 8. Save the Fine-tuned Model ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m                     ):\n\u001b[1;32m   2562\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/inlegalbert-finetuned-legalseg\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",  # Enables wandb logging\n",
    "    run_name=\"inlegalbert-legalseg-finetune\"\n",
    ")\n",
    "\n",
    "# --- 6. Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# --- 7. Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- 8. Save the Fine-tuned Model ---\n",
    "output_dir = \"./final_model/inlegalbert-finetuned-legalseg\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Training complete. Model saved at:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results/inlegalbert-finetuned-legalseg\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=False,\n",
    "#     report_to=\"wandb\",  # Enables wandb logging\n",
    "#     run_name=\"inlegalbert-legalseg-finetune\"\n",
    "# )\n",
    "\n",
    "# # --- 6. Trainer ---\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# # --- 7. Train ---\n",
    "# trainer.train()\n",
    "\n",
    "# # --- 8. Save the Fine-tuned Model ---\n",
    "# output_dir = \"./final_model/inlegalbert-finetuned-legalseg\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# print(\"✅ Training complete. Model saved at:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:29:21.913701Z",
     "iopub.status.busy": "2025-07-14T15:29:21.913397Z",
     "iopub.status.idle": "2025-07-14T15:31:36.363325Z",
     "shell.execute_reply": "2025-07-14T15:31:36.362442Z",
     "shell.execute_reply.started": "2025-07-14T15:29:21.913676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 15:29:36.305712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752506976.495074      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752506976.547371      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ea9ef1fe66458d928d1a143be6fab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e591b7aa2b4dcbb36c4dd7fd8de13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092fb52550c240b29d7678aec0c015f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9240110ce7314a74aa26e5f489e0986d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebafe1638a80416489c2771f1c96d53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/534M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at law-ai/InLegalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ca42e9ef45415c951b8162aac0e023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/137326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c1c33285454f40a1d8707f0dfeac62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/534M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21da0b7e8bae4289a2896744e8b973d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/1204273043.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250714_153116-ghs6vdnz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anshsinghal-bennett-university/huggingface/runs/ghs6vdnz' target=\"_blank\">./results/inlegalbert-finetuned-legalseg</a></strong> to <a href='https://wandb.ai/anshsinghal-bennett-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anshsinghal-bennett-university/huggingface' target=\"_blank\">https://wandb.ai/anshsinghal-bennett-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anshsinghal-bennett-university/huggingface/runs/ghs6vdnz' target=\"_blank\">https://wandb.ai/anshsinghal-bennett-university/huggingface/runs/ghs6vdnz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='25749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   11/25749 00:09 < 7:11:30, 0.99 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1204273043.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Save the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m                     ):\n\u001b[1;32m   2562\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# import torch\n",
    "\n",
    "# # --- 1. Load and Prepare the Dataset ---\n",
    "# # Load your data from the CSV file\n",
    "# try:\n",
    "#     df = pd.read_csv('/kaggle/input/legalseg/val.csv')  # Change this to your actual CSV path\n",
    "#     df = df.dropna()\n",
    "#     df = df.drop(\"Index\", axis=1)\n",
    "#     df.columns = ['text', 'label_str']  # Rename columns for consistency\n",
    "# except FileNotFoundError:\n",
    "#     print(\"CSV file not found. Using a dummy dataframe for demonstration.\")\n",
    "#     # Dummy data for fallback\n",
    "#     data = {\n",
    "#         'text': [\n",
    "#             \"The petitioner argues the contract is invalid due to fraud.\",\n",
    "#             \"The court held the agreement was enforceable.\",\n",
    "#             \"Respondent claims the delay was unavoidable.\",\n",
    "#             \"Facts of the case show negligence by the petitioner.\"\n",
    "#         ],\n",
    "#         'label_str': [\n",
    "#             \"Arguments of Petitioner\",\n",
    "#             \"Decision\",\n",
    "#             \"Arguments of Respondent\",\n",
    "#             \"Facts\"\n",
    "#         ]\n",
    "#     }\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "# # Create label mappings\n",
    "# unique_labels = df['label_str'].unique()\n",
    "# label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "# id2label = {i: label for label, i in label2id.items()}\n",
    "# df['label'] = df['label_str'].map(label2id)\n",
    "\n",
    "# # Convert DataFrame to Hugging Face Dataset\n",
    "# full_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# # Train/test split\n",
    "# train_ds, test_ds = full_dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n",
    "# # --- 2. Load Tokenizer and Model ---\n",
    "# model_name = \"law-ai/InLegalBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=len(unique_labels),\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id\n",
    "# )\n",
    "\n",
    "# # --- 3. Tokenization ---\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# tokenized_train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "# tokenized_test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Set format for PyTorch\n",
    "# tokenized_train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# tokenized_test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# # --- 4. Define Training Configuration ---\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall\n",
    "#     }\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results/inlegalbert-finetuned-legalseg\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=False,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10\n",
    "# )\n",
    "\n",
    "# # --- 5. Train the Model ---\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_ds,\n",
    "#     eval_dataset=tokenized_test_ds,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# model.save_pretrained(\"./final_model/inlegalbert-finetuned-legalseg\")\n",
    "# tokenizer.save_pretrained(\"./final_model/inlegalbert-finetuned-legalseg\")\n",
    "\n",
    "# print(\"Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:58:54.302506Z",
     "iopub.status.busy": "2025-07-14T18:58:54.301812Z",
     "iopub.status.idle": "2025-07-14T18:59:01.223692Z",
     "shell.execute_reply": "2025-07-14T18:59:01.222539Z",
     "shell.execute_reply.started": "2025-07-14T18:58:54.302472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T02:48:34.129339Z",
     "iopub.status.busy": "2025-07-15T02:48:34.129043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at law-ai/InLegalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5470da8fd441fe83cb47d25417186d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/610829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc54df6e2e4140c08dbad7db6cb64ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/1754307305.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='987' max='95450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  987/95450 23:38 < 37:46:46, 0.69 it/s, Epoch 0.10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "# --- 1. Load and Combine CSVs ---\n",
    "train_df = pd.read_csv('/kaggle/input/legalseg/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/legalseg/test.csv')\n",
    "val_df = pd.read_csv('/kaggle/input/legalseg/val.csv')\n",
    "\n",
    "for df in [train_df, test_df, val_df]:\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(\"Index\", axis=1, inplace=True)\n",
    "    df.columns = ['text', 'label_str']\n",
    "\n",
    "# Combine train and test\n",
    "combined_train_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Label mapping\n",
    "unique_labels = pd.concat([combined_train_df, val_df])['label_str'].unique()\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "combined_train_df['label'] = combined_train_df['label_str'].map(label2id)\n",
    "val_df['label'] = val_df['label_str'].map(label2id)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(combined_train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# --- 2. Load Tokenizer and Model ---\n",
    "model_name = \"law-ai/InLegalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# --- 3. Tokenize ---\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# --- 4. Metric Function ---\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# --- 5. Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/inlegalbert-finetuned-legalseg\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",              # Log to wandb\n",
    "    run_name=\"inlegalbert-legalseg-finetune\"\n",
    ")\n",
    "\n",
    "# --- 6. Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 7. Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- 8. Save Best Model to Directory ---\n",
    "output_dir = \"./final_model/inlegalbert-finetuned-legalseg\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# --- 9. Upload Best Model as Artifact to W&B ---\n",
    "artifact = wandb.Artifact(name=\"inlegalbert-legalseg-best\", type=\"model\")\n",
    "artifact.add_dir(output_dir)\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "print(\"✅ Training complete. Best model saved and logged to W&B.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:59:07.280498Z",
     "iopub.status.busy": "2025-07-14T18:59:07.280210Z",
     "iopub.status.idle": "2025-07-14T18:59:07.285570Z",
     "shell.execute_reply": "2025-07-14T18:59:07.284829Z",
     "shell.execute_reply.started": "2025-07-14T18:59:07.280475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from opennyai import Pipeline\n",
    "from opennyai.utils import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:58:32.483472Z",
     "iopub.status.busy": "2025-07-14T18:58:32.483159Z",
     "iopub.status.idle": "2025-07-14T18:58:41.723457Z",
     "shell.execute_reply": "2025-07-14T18:58:41.722762Z",
     "shell.execute_reply.started": "2025-07-14T18:58:32.483450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.11/dist-packages (1.3.9)\n",
      "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (3.8.7)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.3.1)\n",
      "Requirement already satisfied: transformers<4.50.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (4.49.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (0.9.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.6.15)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.33.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers<4.50.0,>=3.4.0->spacy-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers<4.50.0,>=3.4.0->spacy-transformers) (1.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (6.4.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->spacy-transformers) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.2.0\n",
      "  Downloading https://huggingface.co/opennyaiorg/en_legal_ner_trf/resolve/main/STOCK_SPACY_MODELS/en_core_web_trf-3.2.0-py3-none-any.whl (460.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install spacy-transformers\n",
    "!pip install https://huggingface.co/opennyaiorg/en_legal_ner_trf/resolve/main/STOCK_SPACY_MODELS/en_core_web_trf-3.2.0-py3-none-any.whl --no-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:56:32.565782Z",
     "iopub.status.busy": "2025-07-14T18:56:32.565474Z",
     "iopub.status.idle": "2025-07-14T18:56:39.275363Z",
     "shell.execute_reply": "2025-07-14T18:56:39.274424Z",
     "shell.execute_reply.started": "2025-07-14T18:56:32.565755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.2.0\n",
      "  Downloading https://huggingface.co/opennyaiorg/en_legal_ner_trf/resolve/main/STOCK_SPACY_MODELS/en_core_web_trf-3.2.0-py3-none-any.whl (460.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install -U https://huggingface.co/opennyaiorg/en_legal_ner_trf/resolve/main/STOCK_SPACY_MODELS/en_core_web_trf-3.2.0-py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:56:01.326707Z",
     "iopub.status.busy": "2025-07-14T18:56:01.325914Z",
     "iopub.status.idle": "2025-07-14T18:56:05.918089Z",
     "shell.execute_reply": "2025-07-14T18:56:05.917199Z",
     "shell.execute_reply.started": "2025-07-14T18:56:01.326677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install tokenizers>=0.14.1 transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:47:10.688613Z",
     "iopub.status.busy": "2025-07-14T18:47:10.687881Z",
     "iopub.status.idle": "2025-07-14T18:47:13.229998Z",
     "shell.execute_reply": "2025-07-14T18:47:13.229330Z",
     "shell.execute_reply.started": "2025-07-14T18:47:10.688566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.20.1\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n",
      "Collecting tokenizers==0.12.1\n",
      "  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opennyai in /usr/local/lib/python3.11/dist-packages (0.0.13)\n"
     ]
    }
   ],
   "source": [
    "# Install transformers & tokenizers manually\n",
    "!pip install -U 'transformers==4.20.1' 'tokenizers==0.12.1' --prefer-binary --no-build-isolation\n",
    "\n",
    "# Then install opennyai again (should skip transformers/tokenizers now)\n",
    "!pip install -U opennyai --no-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:21:54.054262Z",
     "iopub.status.busy": "2025-07-14T18:21:54.053845Z",
     "iopub.status.idle": "2025-07-14T18:22:10.600316Z",
     "shell.execute_reply": "2025-07-14T18:22:10.599563Z",
     "shell.execute_reply.started": "2025-07-14T18:21:54.054236Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.2.0\n",
      "  Downloading https://huggingface.co/opennyaiorg/en_legal_ner_trf/resolve/main/STOCK_SPACY_MODELS/en_core_web_trf-3.2.0-py3-none-any.whl (460.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy<3.3.0,>=3.2.0 (from en-core-web-trf==3.2.0)\n",
      "  Using cached spacy-3.2.6-cp311-cp311-linux_x86_64.whl\n",
      "Collecting spacy-transformers<1.2.0,>=1.1.2 (from en-core-web-trf==3.2.0)\n",
      "  Using cached spacy_transformers-1.1.9-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.10)\n",
      "Collecting thinc<8.1.0,>=8.0.12 (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0)\n",
      "  Using cached thinc-8.0.17-cp311-cp311-linux_x86_64.whl\n",
      "Collecting blis<0.8.0,>=0.4.0 (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0)\n",
      "  Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0)\n",
      "  Using cached pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (25.0)\n",
      "Collecting typing_extensions<4.6.0,>=3.7.4.1 (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.5.0)\n",
      "INFO: pip is looking at multiple versions of spacy-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting spacy-transformers<1.2.0,>=1.1.2 (from en-core-web-trf==3.2.0)\n",
      "  Using cached spacy_transformers-1.1.8-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "  Using cached spacy_transformers-1.1.7-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting transformers<4.21.0,>=3.4.0 (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0)\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2.1.2)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.9.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.3.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2025.6.15)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (3.18.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (12.5.82)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.33.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2024.11.6)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0)\n",
      "  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (8.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.3.0)\n",
      "Using cached spacy_transformers-1.1.7-py2.py3-none-any.whl (53 kB)\n",
      "Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Using cached pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install -U https://huggingface.co/opennyaiorg/en_legal_ner_trf/resolve/main/STOCK_SPACY_MODELS/en_core_web_trf-3.2.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:20:40.256257Z",
     "iopub.status.busy": "2025-07-14T18:20:40.255700Z",
     "iopub.status.idle": "2025-07-14T18:20:57.912862Z",
     "shell.execute_reply": "2025-07-14T18:20:57.912189Z",
     "shell.execute_reply.started": "2025-07-14T18:20:40.256230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.2.2\n",
      "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy>=1.17.3 (from scikit-learn==1.2.2)\n",
      "  Using cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.3.2 (from scikit-learn==1.2.2)\n",
      "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn==1.2.2)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.6.0\n",
      "    Uninstalling threadpoolctl-3.6.0:\n",
      "      Successfully uninstalled threadpoolctl-3.6.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.5.1\n",
      "    Uninstalling joblib-1.5.1:\n",
      "      Successfully uninstalled joblib-1.5.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.3\n",
      "    Uninstalling scipy-1.15.3:\n",
      "      Successfully uninstalled scipy-1.15.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.2\n",
      "    Uninstalling scikit-learn-1.3.2:\n",
      "      Successfully uninstalled scikit-learn-1.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "opennyai 0.0.13 requires scikit-learn<2.0.0,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "opennyai 0.0.13 requires spacy<3.3.0,>=3.2.2, but you have spacy 3.8.7 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.0 which is incompatible.\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\n",
      "cupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.1 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.0 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
      "gradio 5.31.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.4.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.5.1 numpy-2.3.1 scikit-learn-1.2.2 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.2.2 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:51:21.796749Z",
     "iopub.status.busy": "2025-07-14T18:51:21.796149Z",
     "iopub.status.idle": "2025-07-14T18:51:24.219146Z",
     "shell.execute_reply": "2025-07-14T18:51:24.218198Z",
     "shell.execute_reply.started": "2025-07-14T18:51:21.796716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:51:32.292752Z",
     "iopub.status.busy": "2025-07-14T18:51:32.292446Z",
     "iopub.status.idle": "2025-07-14T18:51:34.386932Z",
     "shell.execute_reply": "2025-07-14T18:51:34.386020Z",
     "shell.execute_reply.started": "2025-07-14T18:51:32.292723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers>=0.14.1 transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:51:58.085797Z",
     "iopub.status.busy": "2025-07-14T18:51:58.085501Z",
     "iopub.status.idle": "2025-07-14T18:52:05.290322Z",
     "shell.execute_reply": "2025-07-14T18:52:05.289427Z",
     "shell.execute_reply.started": "2025-07-14T18:51:58.085772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opennyai in /usr/local/lib/python3.11/dist-packages (0.0.13)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from opennyai) (4.13.4)\n",
      "Requirement already satisfied: levenshtein<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from opennyai) (0.23.0)\n",
      "Requirement already satisfied: multiprocess<0.71.0,>=0.70.15 in /usr/local/lib/python3.11/dist-packages (from opennyai) (0.70.15)\n",
      "Requirement already satisfied: nltk>=3.6 in /usr/local/lib/python3.11/dist-packages (from opennyai) (3.9.1)\n",
      "Requirement already satisfied: pandas>1.2.4 in /usr/local/lib/python3.11/dist-packages (from opennyai) (2.2.3)\n",
      "Requirement already satisfied: prettytable>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from opennyai) (3.16.0)\n",
      "Requirement already satisfied: pytest<8.0.0,>=7.4.3 in /usr/local/lib/python3.11/dist-packages (from opennyai) (7.4.3)\n",
      "Requirement already satisfied: pytorch-transformers<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from opennyai) (1.2.0)\n",
      "Collecting scikit-learn<2.0.0,>=1.3.2 (from opennyai)\n",
      "  Using cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting spacy<3.3.0,>=3.2.2 (from opennyai)\n",
      "  Using cached spacy-3.2.6-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: spacy-transformers>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from opennyai) (1.3.9)\n",
      "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from opennyai) (2.1.2)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from levenshtein<0.24.0,>=0.23.0->opennyai) (3.13.0)\n",
      "Requirement already satisfied: dill>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from multiprocess<0.71.0,>=0.70.15->opennyai) (0.3.8)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<8.0.0,>=7.4.3->opennyai) (2.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest<8.0.0,>=7.4.3->opennyai) (25.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from pytest<8.0.0,>=7.4.3->opennyai) (1.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers<2.0.0,>=1.2.0->opennyai) (2.3.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers<2.0.0,>=1.2.0->opennyai) (1.39.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers<2.0.0,>=1.2.0->opennyai) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers<2.0.0,>=1.2.0->opennyai) (4.67.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers<2.0.0,>=1.2.0->opennyai) (2024.11.6)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers<2.0.0,>=1.2.0->opennyai) (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers<2.0.0,>=1.2.0->opennyai) (0.1.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.3.2->opennyai) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.3.2->opennyai) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.3.2->opennyai) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (3.0.10)\n",
      "Collecting thinc<8.1.0,>=8.0.12 (from spacy<3.3.0,>=3.2.2->opennyai)\n",
      "  Using cached thinc-8.0.17-cp311-cp311-linux_x86_64.whl\n",
      "Collecting blis<0.8.0,>=0.4.0 (from spacy<3.3.0,>=3.2.2->opennyai)\n",
      "  Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (2.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (6.4.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.3.0,>=3.2.2->opennyai)\n",
      "  Using cached pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (80.9.0)\n",
      "Collecting typing_extensions<4.6.0,>=3.7.4.1 (from spacy<3.3.0,>=3.2.2->opennyai)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2->opennyai) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.2->opennyai) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers<2.0.0,>=1.2.0->opennyai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers<2.0.0,>=1.2.0->opennyai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers<2.0.0,>=1.2.0->opennyai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers<2.0.0,>=1.2.0->opennyai) (2025.6.15)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.2->opennyai) (8.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.10.0->opennyai) (2.7)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.2->opennyai) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>1.2.4->opennyai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>1.2.4->opennyai) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>1.2.4->opennyai) (2025.2)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.2->opennyai) (0.1.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable>=3.1.1->opennyai) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>1.2.4->opennyai) (1.17.0)\n",
      "INFO: pip is looking at multiple versions of spacy-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting spacy-transformers>=1.1.4 (from opennyai)\n",
      "  Using cached spacy_transformers-1.3.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "  Using cached spacy_transformers-1.3.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "  Using cached spacy_transformers-1.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "  Using cached spacy_transformers-1.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "  Using cached spacy_transformers-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.1 kB)\n",
      "  Using cached spacy_transformers-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.1 kB)\n",
      "  Using cached spacy_transformers-1.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "INFO: pip is still looking at multiple versions of spacy-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached spacy_transformers-1.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "  Using cached spacy_transformers-1.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "  Using cached spacy_transformers-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "  Using cached spacy_transformers-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "  Using cached spacy_transformers-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached spacy_transformers-1.1.9-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached spacy_transformers-1.1.8-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "  Using cached spacy_transformers-1.1.7-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting transformers<4.21.0,>=3.4.0 (from spacy-transformers>=1.1.4->opennyai)\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers>=1.1.4->opennyai) (0.9.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers>=1.1.4->opennyai) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers>=1.1.4->opennyai) (0.33.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers>=1.1.4->opennyai) (6.0.2)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21.0,>=3.4.0->spacy-transformers>=1.1.4->opennyai)\n",
      "  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers<4.21.0,>=3.4.0->spacy-transformers>=1.1.4->opennyai) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers<4.21.0,>=3.4.0->spacy-transformers>=1.1.4->opennyai) (1.1.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->opennyai) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.1->opennyai) (12.5.82)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3->pytorch-transformers<2.0.0,>=1.2.0->opennyai) (1.39.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->pytorch-transformers<2.0.0,>=1.2.0->opennyai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->pytorch-transformers<2.0.0,>=1.2.0->opennyai) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.3.0,>=3.2.2->opennyai) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.12.1->opennyai) (1.3.0)\n",
      "Using cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Using cached pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Using cached spacy_transformers-1.1.7-py2.py3-none-any.whl (53 kB)\n",
      "Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U opennyai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:12:00.147646Z",
     "iopub.status.busy": "2025-07-14T18:12:00.147304Z",
     "iopub.status.idle": "2025-07-14T18:12:01.879316Z",
     "shell.execute_reply": "2025-07-14T18:12:01.878563Z",
     "shell.execute_reply.started": "2025-07-14T18:12:00.147622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opennyai\n",
      "  Using cached opennyai-0.0.13-py3-none-any.whl.metadata (6.2 kB)\n",
      "Using cached opennyai-0.0.13-py3-none-any.whl (170 kB)\n",
      "Installing collected packages: opennyai\n",
      "Successfully installed opennyai-0.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install opennyai --no-deps  # Avoid pulling old deps that break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:16:15.801363Z",
     "iopub.status.busy": "2025-07-14T18:16:15.800738Z",
     "iopub.status.idle": "2025-07-14T18:18:42.105958Z",
     "shell.execute_reply": "2025-07-14T18:18:42.104888Z",
     "shell.execute_reply.started": "2025-07-14T18:16:15.801334Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy<3.3.0,>=3.2.2\n",
      "  Using cached spacy-3.2.6-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: pandas>1.2.4 in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10.0 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
      "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Collecting levenshtein==0.23.0\n",
      "  Using cached Levenshtein-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting multiprocess==0.70.15\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting scikit-learn==1.3.2\n",
      "  Using cached scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pytest==7.4.3\n",
      "  Using cached pytest-7.4.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: prettytable>=3.1.1 in /usr/local/lib/python3.11/dist-packages (3.16.0)\n",
      "Requirement already satisfied: nltk>=3.6 in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Collecting pytorch-transformers==1.2.0\n",
      "  Using cached pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.1.0 (from levenshtein==0.23.0)\n",
      "  Using cached rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dill>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from multiprocess==0.70.15) (0.3.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (3.6.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.3) (2.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.3) (25.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.3) (1.6.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers==1.2.0) (1.39.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers==1.2.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers==1.2.0) (4.67.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers==1.2.0) (2024.11.6)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from pytorch-transformers==1.2.0) (0.2.0)\n",
      "Collecting sacremoses (from pytorch-transformers==1.2.0)\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (3.0.10)\n",
      "Collecting thinc<8.1.0,>=8.0.12 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached thinc-8.0.17-cp311-cp311-linux_x86_64.whl\n",
      "Collecting blis<0.8.0,>=0.4.0 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (2.0.10)\n",
      "Collecting typer<0.5.0,>=0.3.0 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached typer-0.4.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pathy>=0.3.5 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (75.2.0)\n",
      "Collecting typing_extensions<4.6.0,>=3.7.4.1 (from spacy<3.3.0,>=3.2.2)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.3.0,>=3.2.2) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>1.2.4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>1.2.4) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>1.2.4) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.10.0) (2.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1) (3.18.0)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch>=1.12.1\n",
      "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "  Using cached torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "  Using cached torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "  Using cached torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "  Using cached torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "  Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "  Using cached torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1) (3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.12.1)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.12.1)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch>=1.12.1)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.12.1)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch>=1.12.1)\n",
      "  Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.1) (12.5.82)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable>=3.1.1) (0.2.13)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.6) (8.2.1)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.2) (1.3.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2.4.1)\n",
      "Collecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.3.0,>=3.2.2)\n",
      "  Using cached pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>1.2.4) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers==1.2.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers==1.2.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers==1.2.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pytorch-transformers==1.2.0) (2025.6.15)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.39.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.3.0,>=3.2.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.12.1) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.2) (1.2.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\n",
      "Using cached Levenshtein-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
      "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Using cached scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "Using cached pytest-7.4.3-py3-none-any.whl (325 kB)\n",
      "Using cached pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
      "Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Using cached pathy-0.11.0-py3-none-any.whl (47 kB)\n",
      "Using cached pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
      "Using cached pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "Using cached rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Using cached typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Installing collected packages: wasabi, typing_extensions, typer, triton, smart-open, sacremoses, rapidfuzz, pytest, pathlib-abc, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, pydantic, pathy, nvidia-cusolver-cu12, nvidia-cudnn-cu12, levenshtein, torch, blis, thinc, spacy, scikit-learn, pytorch-transformers\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 1.1.3\n",
      "    Uninstalling wasabi-1.1.3:\n",
      "      Successfully uninstalled wasabi-1.1.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.16.0\n",
      "    Uninstalling typer-0.16.0:\n",
      "      Successfully uninstalled typer-0.16.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.1.0\n",
      "    Uninstalling smart-open-7.1.0:\n",
      "      Successfully uninstalled smart-open-7.1.0\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 8.3.5\n",
      "    Uninstalling pytest-8.3.5:\n",
      "      Successfully uninstalled pytest-8.3.5\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.7\n",
      "    Uninstalling pydantic-2.11.7:\n",
      "      Successfully uninstalled pydantic-2.11.7\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.3.0\n",
      "    Uninstalling blis-1.3.0:\n",
      "      Successfully uninstalled blis-1.3.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.6\n",
      "    Uninstalling thinc-8.3.6:\n",
      "      Successfully uninstalled thinc-8.3.6\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.7\n",
      "    Uninstalling spacy-3.8.7:\n",
      "      Successfully uninstalled spacy-3.8.7\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opennyai 0.0.13 requires spacy-transformers>=1.1.4, which is not installed.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\n",
      "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "typing-inspection 0.4.1 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "sigstore 3.6.4 requires pydantic<3,>=2, but you have pydantic 1.8.2 which is incompatible.\n",
      "sigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.8.2 which is incompatible.\n",
      "pyopenssl 25.1.0 requires typing-extensions>=4.9; python_version < \"3.13\" and python_version >= \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires pydantic>=2, but you have pydantic 1.8.2 which is incompatible.\n",
      "docstring-to-markdown 0.17 requires typing_extensions>=4.6, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "onnx 1.18.0 requires typing_extensions>=4.7.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "alembic 1.16.2 requires typing-extensions>=4.12, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "plum-dispatch 2.5.7 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "openai 1.91.0 requires pydantic<3,>=1.9.0, but you have pydantic 1.8.2 which is incompatible.\n",
      "openai 1.91.0 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "ibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\n",
      "google-genai 1.21.1 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.8.2 which is incompatible.\n",
      "google-genai 1.21.1 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
      "optree 0.16.0 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "langchain 0.3.26 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.8.2 which is incompatible.\n",
      "fastapi 0.115.13 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\n",
      "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.8.2 which is incompatible.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
      "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "wandb 0.20.1 requires typing-extensions<5,>=4.8, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "gradio 5.31.0 requires pydantic<2.12,>=2.0, but you have pydantic 1.8.2 which is incompatible.\n",
      "gradio 5.31.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.4.2 which is incompatible.\n",
      "langchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "langchain-core 0.3.66 requires pydantic>=2.7.4, but you have pydantic 1.8.2 which is incompatible.\n",
      "langchain-core 0.3.66 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "sqlalchemy 2.0.41 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-0.7.11 levenshtein-0.23.0 multiprocess-0.70.15 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.8.2 pytest-7.4.3 pytorch-transformers-1.2.0 rapidfuzz-3.13.0 sacremoses-0.1.1 scikit-learn-1.3.2 smart-open-6.4.0 spacy-3.2.6 thinc-8.0.17 torch-2.1.2 triton-2.1.0 typer-0.4.2 typing_extensions-4.5.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "  \"spacy>=3.2.2,<3.3.0\" \\\n",
    "  \"pandas>1.2.4\" \\\n",
    "  \"beautifulsoup4>=4.10.0\" \\\n",
    "  \"torch>=1.12.1\" \\\n",
    "  \"levenshtein==0.23.0\" \\\n",
    "  \"multiprocess==0.70.15\" \\\n",
    "  \"scikit-learn==1.3.2\" \\\n",
    "  \"pytest==7.4.3\" \\\n",
    "  \"prettytable>=3.1.1\" \\\n",
    "  \"nltk>=3.6\" \\\n",
    "  \"pytorch-transformers==1.2.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:18:45.860953Z",
     "iopub.status.busy": "2025-07-14T18:18:45.860113Z",
     "iopub.status.idle": "2025-07-14T18:19:09.912111Z",
     "shell.execute_reply": "2025-07-14T18:19:09.911387Z",
     "shell.execute_reply.started": "2025-07-14T18:18:45.860907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "scikit-learn 1.3.2 requires numpy<2.0,>=1.17.3, but you have numpy 2.2.6 which is incompatible.\n",
      "opennyai 0.0.13 requires spacy<3.3.0,>=3.2.2, but you have spacy 3.8.7 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "ibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
      "gradio 5.31.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.4.2 which is incompatible.\n",
      "langchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install spacy-transformers>=1.1.4 --prefer-binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T17:51:28.021840Z",
     "iopub.status.busy": "2025-07-14T17:51:28.021550Z",
     "iopub.status.idle": "2025-07-14T17:51:43.969859Z",
     "shell.execute_reply": "2025-07-14T17:51:43.969130Z",
     "shell.execute_reply.started": "2025-07-14T17:51:28.021817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cupy\n",
      "  Downloading cupy-13.5.1.tar.gz (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy<2.6,>=1.22 in /usr/local/lib/python3.11/dist-packages (from cupy) (1.26.4)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy) (0.8.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.6,>=1.22->cupy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.6,>=1.22->cupy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.6,>=1.22->cupy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.6,>=1.22->cupy) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.6,>=1.22->cupy) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.6,>=1.22->cupy) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.6,>=1.22->cupy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.6,>=1.22->cupy) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.6,>=1.22->cupy) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.6,>=1.22->cupy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.6,>=1.22->cupy) (2024.2.0)\n",
      "Building wheels for collected packages: cupy\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for cupy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for cupy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31m  ERROR: Failed building wheel for cupy\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build cupy\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (cupy)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T17:49:53.421667Z",
     "iopub.status.busy": "2025-07-14T17:49:53.421416Z",
     "iopub.status.idle": "2025-07-14T17:49:53.652155Z",
     "shell.execute_reply": "2025-07-14T17:49:53.650998Z",
     "shell.execute_reply.started": "2025-07-14T17:49:53.421641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T06:15:32.924054Z",
     "iopub.status.busy": "2025-07-15T06:15:32.923402Z",
     "iopub.status.idle": "2025-07-15T06:15:33.041672Z",
     "shell.execute_reply": "2025-07-15T06:15:33.040934Z",
     "shell.execute_reply.started": "2025-07-15T06:15:32.924032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/DHARA/rrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T06:16:02.945172Z",
     "iopub.status.busy": "2025-07-15T06:16:02.944790Z",
     "iopub.status.idle": "2025-07-15T06:16:02.950021Z",
     "shell.execute_reply": "2025-07-15T06:16:02.949376Z",
     "shell.execute_reply.started": "2025-07-15T06:16:02.945145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[project]\n",
      "name = \"working\"\n",
      "version = \"0.1.0\"\n",
      "description = \"Add your description here\"\n",
      "readme = \"README.md\"\n",
      "requires-python = \">=3.10\"\n",
      "dependencies = [\n",
      "    \"beautifulsoup4>=4.10.0\",\n",
      "    \"cupy-cuda12x>=13.5.1\",\n",
      "    \"levenshtein==0.23.0\",\n",
      "    \"multiprocess==0.70.15\",\n",
      "    \"nltk>=3.6\",\n",
      "    \"opennyai>=0.0.13\",\n",
      "    \"pandas>1.2.4\",\n",
      "    \"prettytable>=3.1.1\",\n",
      "    \"pytest==7.4.3\",\n",
      "    \"pytorch-transformers==1.2.0\",\n",
      "    \"scikit-learn==1.3.2\",\n",
      "    \"spacy>=3.2.2,<3.3.0\",\n",
      "    \"spacy-transformers>=1.1.4,<1.2.0\",\n",
      "    \"torch>=1.12.1\",\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/working/pyproject.toml\", 'r') as f:\n",
    "    all = f.read()\n",
    "print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T06:25:39.912583Z",
     "iopub.status.busy": "2025-07-15T06:25:39.912296Z",
     "iopub.status.idle": "2025-07-15T06:25:40.257104Z",
     "shell.execute_reply": "2025-07-15T06:25:40.256352Z",
     "shell.execute_reply.started": "2025-07-15T06:25:39.912562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DHARA  main.py\tREADME.md  test.py\n"
     ]
    }
   ],
   "source": [
    "!rm /kaggle/working/uv.lock\n",
    "!rm /kaggle/working/pyproject.toml\n",
    "!ls /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T07:29:00.945691Z",
     "iopub.status.busy": "2025-07-15T07:29:00.944809Z",
     "iopub.status.idle": "2025-07-15T07:29:01.065069Z",
     "shell.execute_reply": "2025-07-15T07:29:01.064305Z",
     "shell.execute_reply.started": "2025-07-15T07:29:00.945659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T07:53:23.334705Z",
     "iopub.status.busy": "2025-07-15T07:53:23.334286Z",
     "iopub.status.idle": "2025-07-15T07:54:41.059256Z",
     "shell.execute_reply": "2025-07-15T07:54:41.058503Z",
     "shell.execute_reply.started": "2025-07-15T07:53:23.334685Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[1A\u001b[2mInstalled \u001b[1mPython 3.10.16\u001b[0m \u001b[2min 1.76s\u001b[0m\u001b[0m-------------------\u001b[2m\u001b[0m\u001b[0m 19.77 MiB/19.81 MiB                                                                             \u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\n",
      " \u001b[32m+\u001b[39m \u001b[1mcpython-3.10.16-linux-x86_64-gnu\u001b[0m\n",
      "Pinned `\u001b[36m.python-version\u001b[39m` to `\u001b[32m3.10\u001b[39m`\n",
      "Initialized project `\u001b[36mworking\u001b[39m`\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m97 packages\u001b[0m \u001b[2min 60ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `spacy==3.2.6` does not have an extra named `cuda-autodetect`\u001b[0m\n",
      "\u001b[2K\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `spacy==3.2.6` does not have an extra named `cuda-autodetect`\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m92 packages\u001b[0m \u001b[2min 0.03ms\u001b[0m\u001b[0m\n",
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "!uv python install 3.10\n",
    "!uv python pin 3.10\n",
    "!uv init --python 3.10\n",
    "!uv add -q opennyai\n",
    "!uv add -q cupy-cuda12x\n",
    "# Recommended: Install spaCy with CUDA auto-detection\n",
    "!uv add \"spacy[cuda-autodetect]\"\n",
    "!uv run python --version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T07:41:22.082569Z",
     "iopub.status.busy": "2025-07-15T07:41:22.081798Z",
     "iopub.status.idle": "2025-07-15T07:41:23.976264Z",
     "shell.execute_reply": "2025-07-15T07:41:23.975348Z",
     "shell.execute_reply.started": "2025-07-15T07:41:22.082538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: spacy\n",
      "Version: 3.8.7\n",
      "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
      "Home-page: https://spacy.io\n",
      "Author: Explosion\n",
      "Author-email: contact@explosion.ai\n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\n",
      "Required-by: fastai\n"
     ]
    }
   ],
   "source": [
    "!uv run pip show spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install the Rust compiler and its package manager, Cargo\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] += \":/root/.cargo/bin\"\n",
    "\n",
    "# !uv add -q \"numpy<2\"\n",
    "!pip install opennyai\n",
    "!pip install cupy-cuda12x\n",
    "!pip install -U \"spacy[cuda12x]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:41:33.550737Z",
     "iopub.status.busy": "2025-07-15T08:41:33.550494Z",
     "iopub.status.idle": "2025-07-15T08:42:27.033763Z",
     "shell.execute_reply": "2025-07-15T08:42:27.032683Z",
     "shell.execute_reply.started": "2025-07-15T08:41:33.550714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Install Rust (for the 'tokenizers' dependency)\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ['PATH'] += \":/root/.cargo/bin\"\n",
    "\n",
    "# 2. Configure the system's dynamic linker to find CUDA libraries\n",
    "# This makes the libraries available globally for the session.\n",
    "!echo \"/usr/local/cuda/lib64\" >> /etc/ld.so.conf.d/cuda.conf\n",
    "!ldconfig\n",
    "\n",
    "# 3. Set up uv and install all packages in one command\n",
    "!uv python install 3.10\n",
    "!uv python pin 3.10\n",
    "!uv init --python 3.10\n",
    "!uv add -q opennyai \"spacy[cuda12x]\" \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:43:58.113262Z",
     "iopub.status.busy": "2025-07-15T08:43:58.112964Z",
     "iopub.status.idle": "2025-07-15T08:44:04.169735Z",
     "shell.execute_reply": "2025-07-15T08:44:04.169001Z",
     "shell.execute_reply.started": "2025-07-15T08:43:58.113236Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Script Output ---\n",
      "spaCy GPU support: False\n",
      "❌ GPU support for spaCy is still not available.\n",
      "\n",
      "--- Errors ---\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 19, in <module>\n",
      "AssertionError: spaCy GPU not available\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Define the Python code you want to run\n",
    "# Crucially, we set the LD_LIBRARY_PATH inside the code being executed\n",
    "python_code = \"\"\"\n",
    "import os\n",
    "# Point to the location of Kaggle's CUDA shared libraries\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
    "\n",
    "import spacy\n",
    "import cupy\n",
    "\n",
    "# This should now return True\n",
    "is_gpu_available = spacy.prefer_gpu()\n",
    "print(f\"spaCy GPU support: {is_gpu_available}\")\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_id = cupy.cuda.runtime.getDevice()\n",
    "    print(f\"✅ spaCy is now using GPU ID: {gpu_id}\")\n",
    "else:\n",
    "    print(\"❌ GPU support for spaCy is still not available.\")\n",
    "\n",
    "assert is_gpu_available, \"spaCy GPU not available\"\n",
    "\"\"\"\n",
    "\n",
    "# Use `uv run` to execute the code within the virtual environment\n",
    "# The `-c` flag allows passing the code as a string\n",
    "completed_process = subprocess.run([\"uv\", \"run\", \"python\", \"-c\", python_code], capture_output=True, text=True)\n",
    "\n",
    "# Print the output and any errors\n",
    "print(\"--- Script Output ---\")\n",
    "print(completed_process.stdout)\n",
    "if completed_process.stderr:\n",
    "    print(\"--- Errors ---\")\n",
    "    print(completed_process.stderr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T07:57:27.039966Z",
     "iopub.status.busy": "2025-07-15T07:57:27.039638Z",
     "iopub.status.idle": "2025-07-15T07:57:30.072541Z",
     "shell.execute_reply": "2025-07-15T07:57:30.071861Z",
     "shell.execute_reply.started": "2025-07-15T07:57:27.039935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy GPU support: False\n",
      "GPU not available for spaCy. Check your CUDA installation and package versions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/temp.py\", line 18, in <module>\n",
      "    assert is_gpu_available, \"spaCy GPU is still not available.\"\n",
      "AssertionError: spaCy GPU is still not available.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "code = \"\"\"\n",
    "import spacy\n",
    "import cupy\n",
    "\n",
    "# This will now set spaCy to use the GPU and return True\n",
    "is_gpu_available = spacy.prefer_gpu()\n",
    "\n",
    "print(f\"spaCy GPU support: {is_gpu_available}\")\n",
    "\n",
    "if is_gpu_available:\n",
    "    # Get the ID of the current GPU device used by CuPy\n",
    "    gpu_id = cupy.cuda.runtime.getDevice()\n",
    "    print(f\"spaCy is using GPU: {gpu_id}\")\n",
    "else:\n",
    "    print(\"GPU not available for spaCy. Check your CUDA installation and package versions.\")\n",
    "\n",
    "# This assertion should now pass without error\n",
    "assert is_gpu_available, \"spaCy GPU is still not available.\"\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Version info:\", sys.version_info)\n",
    "\n",
    "import cupy\n",
    "print(\"cupy.cuda.runtime.getDeviceCount()\")\n",
    "\n",
    "print(cupy.cuda.runtime.getDeviceCount())\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "import spacy\n",
    "# from spacy.util import use_gpu\n",
    "\n",
    "print(\"spaCy GPU support:\", spacy.prefer_gpu())\n",
    "# print(\"spaCy is using GPU:\", use_gpu())\n",
    "import spacy\n",
    "assert spacy.prefer_gpu() == True, \"spaCy GPU not available\"\n",
    "print(\"✅ spaCy GPU is available\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"temp.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "!uv run temp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:21:26.336693Z",
     "iopub.status.busy": "2025-07-15T08:21:26.336358Z",
     "iopub.status.idle": "2025-07-15T08:21:35.086316Z",
     "shell.execute_reply": "2025-07-15T08:21:35.085621Z",
     "shell.execute_reply.started": "2025-07-15T08:21:26.336666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy GPU support: True\n",
      "✅ spaCy is now using GPU ID: 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import cupy\n",
    "\n",
    "# This will now set spaCy to use the GPU and return True\n",
    "is_gpu_available = spacy.prefer_gpu()\n",
    "\n",
    "print(f\"spaCy GPU support: {is_gpu_available}\")\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_id = cupy.cuda.runtime.getDevice()\n",
    "    print(f\"✅ spaCy is now using GPU ID: {gpu_id}\")\n",
    "else:\n",
    "    print(\"❌ GPU support for spaCy is still not available. Check the installation logs.\")\n",
    "\n",
    "# This assertion should now pass without error\n",
    "assert is_gpu_available, \"spaCy GPU not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-15T07:59:56.167Z",
     "iopub.execute_input": "2025-07-15T07:58:50.458528Z",
     "iopub.status.busy": "2025-07-15T07:58:50.457730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opennyai\n",
      "  Downloading opennyai-0.0.13-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from opennyai) (4.13.4)\n",
      "Collecting levenshtein<0.24.0,>=0.23.0 (from opennyai)\n",
      "  Downloading Levenshtein-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: multiprocess<0.71.0,>=0.70.15 in /usr/local/lib/python3.11/dist-packages (from opennyai) (0.70.16)\n",
      "Requirement already satisfied: nltk>=3.6 in /usr/local/lib/python3.11/dist-packages (from opennyai) (3.9.1)\n",
      "Requirement already satisfied: pandas>1.2.4 in /usr/local/lib/python3.11/dist-packages (from opennyai) (2.2.3)\n",
      "Requirement already satisfied: prettytable>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from opennyai) (3.16.0)\n",
      "Collecting pytest<8.0.0,>=7.4.3 (from opennyai)\n",
      "  Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pytorch-transformers<2.0.0,>=1.2.0 (from opennyai)\n",
      "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting scikit-learn<2.0.0,>=1.3.2 (from opennyai)\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting spacy<3.3.0,>=3.2.2 (from opennyai)\n",
      "  Downloading spacy-3.2.6.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install opennyai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "\n",
    "!uv python install 3.10\n",
    "\n",
    "!uv init --python 3.10\n",
    "\n",
    "!uv python pin 3.10\n",
    "\n",
    "# !uv venv\n",
    "\n",
    "# STEP 1: Create or upgrade virtual env (already done)\n",
    "# !python3 -m venv /kaggle/working/.venv\n",
    "# !/kaggle/working/.venv/bin/python3 -m ensurepip --upgrade\n",
    "\n",
    "# STEP 2: Activate the venv\n",
    "# !source .venv/bin/activate\n",
    "# !source /kaggle/working/.venv/bin/activate\n",
    "\n",
    "!uv pip install \"spacy>=3.2.2,<3.3.0\" \"pandas>1.2.4\" \"beautifulsoup4>=4.10.0\" \"torch>=1.12.1\" \"Levenshtein==0.23.0\" \"multiprocess==0.70.15\" \"spacy-transformers>=1.1.4,<1.2.0\" \"scikit-learn==1.3.2\" \"pytest==7.4.3\" \"prettytable>=3.1.1\" \"nltk>=3.6\" \"pytorch-transformers==1.2.0\" \"opennyai\"\n",
    "\n",
    "\n",
    "# STEP 3: Add required CUDA-enabled packages\n",
    "!uv add cupy-cuda12x\n",
    "\n",
    "# STEP 4: Install spaCy with CUDA support\n",
    "!uv pip install -U \"spacy[cuda12x]\"\n",
    "\n",
    "# STEP 5: Install spaCy-transformers and OpenNyAI\n",
    "\n",
    "# !uv pip install spacy-transformers opennyai\n",
    "\n",
    "# STEP 6: Download the transformer-enabled spaCy model\n",
    "!/kaggle/working/.venv/bin/python3 -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T05:48:25.784971Z",
     "iopub.status.busy": "2025-07-15T05:48:25.784305Z",
     "iopub.status.idle": "2025-07-15T05:48:25.902647Z",
     "shell.execute_reply": "2025-07-15T05:48:25.902017Z",
     "shell.execute_reply.started": "2025-07-15T05:48:25.784942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DHARA' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AnshSinghal/DHARA.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T05:48:34.077755Z",
     "iopub.status.busy": "2025-07-15T05:48:34.076978Z",
     "iopub.status.idle": "2025-07-15T05:48:34.082558Z",
     "shell.execute_reply": "2025-07-15T05:48:34.081809Z",
     "shell.execute_reply.started": "2025-07-15T05:48:34.077726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from opennyai import Pipeline\n",
    "from opennyai.utils import Data\n",
    "\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "\n",
    "# Paths\n",
    "input_dir = \"/kaggle/working/DHARA/cases\"\n",
    "output_dir = \"/kaggle/working/DHARA/rrl\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Collect and read all .txt case files\n",
    "file_paths = sorted(glob(os.path.join(input_dir, \"*.txt\")))\n",
    "texts = []\n",
    "doc_ids = []\n",
    "\n",
    "for path in file_paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        texts.append(f.read())\n",
    "        doc_ids.append(os.path.splitext(os.path.basename(path))[0])\n",
    "\n",
    "# 2. Wrap with OpenNyAI-compatible Data object\n",
    "data = Data(texts, use_gpu=True)\n",
    "\n",
    "# 3. Init pipeline (use GPU if available)\n",
    "pipe = Pipeline(components=[\"Rhetorical_Role\"], use_gpu=True)\n",
    "\n",
    "# 4. Run RRL\n",
    "results = pipe(data)\n",
    "\n",
    "# 5. Save each result as individual JSON file\n",
    "for doc_id, result in zip(doc_ids, results):\n",
    "    output_path = os.path.join(output_dir, f\"{doc_id}.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Inference complete. {len(results)} files saved to {output_dir}\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"DHARA/rrl_run.py\", \"w\") as f:\n",
    "    f.write(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T05:56:07.407940Z",
     "iopub.status.busy": "2025-07-15T05:56:07.407645Z",
     "iopub.status.idle": "2025-07-15T05:56:55.790412Z",
     "shell.execute_reply": "2025-07-15T05:56:55.789705Z",
     "shell.execute_reply.started": "2025-07-15T05:56:07.407914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!/kaggle/working/.venv/bin/python3 -m ensurepip --upgrade\n",
    "\n",
    "\n",
    "!nvcc --version\n",
    "\n",
    "!uv add cupy-cuda12x\n",
    "\n",
    "\n",
    "!uv pip install \"spacy[cuda12x]\"\n",
    "\n",
    "\n",
    "!uv run DHARA/rrl_run.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-15T06:49:27.627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7865303,
     "sourceId": 12467392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
